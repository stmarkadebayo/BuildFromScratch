{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.2 Experimental Lab: Multimodal Fusion Breakdown\n",
    "\n",
    "**Goal:** CLIP-style alignment â†’ swap modalities â†’ observe confusion patterns\n",
    "\n",
    "**Break it:** Misaligned training pairs â†’ measure semantic drift\n",
    "\n",
    "**Visualize:** Embedding space t-SNE plots, cross-modal similarity matrices\n",
    "\n",
    "**Optimize:** Contrastive learning improvements, modality alignment techniques\n",
    "\n",
    "---\n",
    "\n",
    "## Professor's Notes\n",
    "\n",
    "St. Mark, multimodal systems are powerful but fragile. When you combine text and images:\n",
    "- **Alignment matters** - Misaligned pairs create semantic drift\n",
    "- **Modalities interfere** - Corruption in one modality affects the other\n",
    "- **Fusion is lossy** - Information gets lost in translation between modalities\n",
    "\n",
    "Understanding these failure modes helps you build more reliable multimodal systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ”¬ Phase 4.2: Multimodal Fusion Breakdown\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build CLIP-Style Multimodal System\n",
    "\n",
    "Let's create a simplified CLIP-like system that aligns text and image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIPSystem:\n",
    "    \"\"\"Simplified CLIP-like multimodal alignment system\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 128, temperature: float = 0.07):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temperature = temperature\n",
    "        self.text_embeddings = None\n",
    "        self.image_embeddings = None\n",
    "        self.text_descriptions = []\n",
    "        self.image_labels = []\n",
    "        \n",
    "    def add_paired_data(self, text_embeddings: np.ndarray, image_embeddings: np.ndarray,\n",
    "                       text_descriptions: List[str], image_labels: List[str]):\n",
    "        \"\"\"Add paired text-image data\"\"\"\n",
    "        self.text_embeddings = text_embeddings.copy()\n",
    "        self.image_embeddings = image_embeddings.copy()\n",
    "        self.text_descriptions = text_descriptions\n",
    "        self.image_labels = image_labels\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        self.text_embeddings = self.text_embeddings / np.linalg.norm(self.text_embeddings, axis=1, keepdims=True)\n",
    "        self.image_embeddings = self.image_embeddings / np.linalg.norm(self.image_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        print(f\"âœ… Added {len(text_descriptions)} text-image pairs\")\n",
    "        \n",
    "    def compute_similarity_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Compute cross-modal similarity matrix\"\"\"\n",
    "        return cosine_similarity(self.text_embeddings, self.image_embeddings)\n",
    "    \n",
    "    def retrieve_images(self, text_query: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Retrieve most similar images for text query\"\"\"\n",
    "        similarities = cosine_similarity(text_query.reshape(1, -1), self.image_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        return list(zip(top_k_indices, similarities[top_k_indices]))\n",
    "    \n",
    "    def retrieve_texts(self, image_query: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Retrieve most similar texts for image query\"\"\"\n",
    "        similarities = cosine_similarity(image_query.reshape(1, -1), self.text_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        return list(zip(top_k_indices, similarities[top_k_indices]))\n",
    "    \n",
    "    def corrupt_modality(self, modality: str, corruption_type: str, noise_level: float) -> np.ndarray:\n",
    "        \"\"\"Corrupt a specific modality\"\"\"\n",
    "        if modality == 'text':\n",
    "            embeddings = self.text_embeddings.copy()\n",
    "        elif modality == 'image':\n",
    "            embeddings = self.image_embeddings.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Modality must be 'text' or 'image'\")\n",
    "            \n",
    "        if corruption_type == 'gaussian':\n",
    "            noise = np.random.normal(0, noise_level, embeddings.shape)\n",
    "            embeddings += noise\n",
    "        elif corruption_type == 'swap':\n",
    "            # Randomly swap embeddings (misalignment)\n",
    "            indices = np.random.permutation(len(embeddings))\n",
    "            embeddings = embeddings[indices]\n",
    "        elif corruption_type == 'zero_out':\n",
    "            # Zero out random dimensions\n",
    "            mask = np.random.random(embeddings.shape) < noise_level\n",
    "            embeddings[mask] = 0\n",
    "            \n",
    "        # Renormalize\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        norms = np.maximum(norms, 1e-8)\n",
    "        embeddings = embeddings / norms\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic multimodal data\n",
    "def create_multimodal_data(n_pairs: int = 200, embedding_dim: int = 128):\n",
    "    \"\"\"Create synthetic text-image paired data\"\"\"\n",
    "    \n",
    "    # Create semantic clusters (e.g., animals, vehicles, food, etc.)\n",
    "    categories = ['animals', 'vehicles', 'food', 'buildings', 'nature']\n",
    "    n_categories = len(categories)\n",
    "    \n",
    "    # Generate base embeddings for each category\n",
    "    category_centers = np.random.normal(0, 1, (n_categories, embedding_dim))\n",
    "    \n",
    "    text_embeddings = []\n",
    "    image_embeddings = []\n",
    "    text_descriptions = []\n",
    "    image_labels = []\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        # Generate paired embeddings for this category\n",
    "        n_samples = n_pairs // n_categories\n",
    "        \n",
    "        # Text embeddings (descriptions)\n",
    "        text_center = category_centers[i] + np.random.normal(0, 0.2, embedding_dim)\n",
    "        text_emb, _ = make_blobs(n_samples=n_samples, centers=[text_center], \n",
    "                                cluster_std=0.3, random_state=42+i)\n",
    "        \n",
    "        # Image embeddings (visual features)\n",
    "        image_center = category_centers[i] + np.random.normal(0, 0.2, embedding_dim)\n",
    "        image_emb, _ = make_blobs(n_samples=n_samples, centers=[image_center], \n",
    "                                 cluster_std=0.3, random_state=142+i)\n",
    "        \n",
    "        text_embeddings.append(text_emb)\n",
    "        image_embeddings.append(image_emb)\n",
    "        \n",
    "        # Create descriptions and labels\n",
    "        for j in range(n_samples):\n",
    "            text_descriptions.append(f\"A photo of {category}\")\n",
    "            image_labels.append(f\"{category}_image_{j}\")\n",
    "    \n",
    "    # Stack all embeddings\n",
    "    text_embeddings = np.vstack(text_embeddings)\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    \n",
    "    # Add some cross-modal alignment (CLIP-like)\n",
    "    # Make corresponding text-image pairs more similar\n",
    "    alignment_strength = 0.3\n",
    "    for i in range(len(text_embeddings)):\n",
    "        # Average the embeddings to create alignment\n",
    "        avg_emb = (text_embeddings[i] + image_embeddings[i]) / 2\n",
    "        text_embeddings[i] = text_embeddings[i] * (1 - alignment_strength) + avg_emb * alignment_strength\n",
    "        image_embeddings[i] = image_embeddings[i] * (1 - alignment_strength) + avg_emb * alignment_strength\n",
    "    \n",
    "    return text_embeddings, image_embeddings, text_descriptions, image_labels, categories\n",
    "\n",
    "# Create multimodal data\n",
    "text_emb, image_emb, text_desc, image_labels, categories = create_multimodal_data()\n",
    "\n",
    "print(f\"ðŸ“ Created {len(text_desc)} text-image pairs\")\n",
    "print(f\"ðŸ“Š Categories: {categories}\")\n",
    "print(f\"ðŸ”¢ Embedding dimension: {text_emb.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP system\n",
    "clip_system = SimpleCLIPSystem()\n",
    "clip_system.add_paired_data(text_emb, image_emb, text_desc, image_labels)\n",
    "\n",
    "# Test clean retrieval\n",
    "test_text_idx = 0\n",
    "test_text_emb = text_emb[test_text_idx:test_text_idx+1]\n",
    "test_image_idx = 0\n",
    "test_image_emb = image_emb[test_image_idx:test_image_idx+1]\n",
    "\n",
    "print(\"\\nðŸ” Clean Multimodal Retrieval:\")\n",
    "print(f\"Query text: {text_desc[test_text_idx]}\")\n",
    "\n",
    "# Text-to-image retrieval\n",
    "image_results = clip_system.retrieve_images(test_text_emb, k=3)\n",
    "print(\"Top 3 retrieved images:\")\n",
    "for rank, (img_idx, score) in enumerate(image_results, 1):\n",
    "    print(f\"  {rank}. {image_labels[img_idx]} (score: {score:.4f})\")\n",
    "\n",
    "# Image-to-text retrieval\n",
    "text_results = clip_system.retrieve_texts(test_image_emb, k=3)\n",
    "print(\"\\nTop 3 retrieved texts:\")\n",
    "for rank, (txt_idx, score) in enumerate(text_results, 1):\n",
    "    print(f\"  {rank}. {text_desc[txt_idx]} (score: {score:.4f})\")\n",
    "\n",
    "# Show similarity matrix\n",
    "similarity_matrix = clip_system.compute_similarity_matrix()\n",
    "print(f\"\\nðŸ“Š Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"ðŸ”¥ Max similarity: {similarity_matrix.max():.4f}\")\n",
    "print(f\"ðŸ¥¶ Min similarity: {similarity_matrix.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Break It: Modality Misalignment & Corruption\n",
    "\n",
    "Now let's systematically break the multimodal alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_performance(\n",
    "    system: SimpleCLIPSystem, \n",
    "    n_test_queries: int = 50\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate multimodal retrieval performance\"\"\"\n",
    "    \n",
    "    text_to_image_precisions = []\n",
    "    image_to_text_precisions = []\n",
    "    \n",
    "    # Test text-to-image retrieval\n",
    "    for i in range(min(n_test_queries, len(system.text_embeddings))):\n",
    "        query_emb = system.text_embeddings[i:i+1]\n",
    "        results = system.retrieve_images(query_emb, k=5)\n",
    "        \n",
    "        # Check if correct image is in top results (simplified: same index)\n",
    "        correct_retrieved = any(idx == i for idx, _ in results)\n",
    "        text_to_image_precisions.append(1.0 if correct_retrieved else 0.0)\n",
    "    \n",
    "    # Test image-to-text retrieval\n",
    "    for i in range(min(n_test_queries, len(system.image_embeddings))):\n",
    "        query_emb = system.image_embeddings[i:i+1]\n",
    "        results = system.retrieve_texts(query_emb, k=5)\n",
    "        \n",
    "        # Check if correct text is in top results\n",
    "        correct_retrieved = any(idx == i for idx, _ in results)\n",
    "        image_to_text_precisions.append(1.0 if correct_retrieved else 0.0)\n",
    "    \n",
    "    return {\n",
    "        'text_to_image_recall@5': np.mean(text_to_image_precisions),\n",
    "        'image_to_text_recall@5': np.mean(image_to_text_precisions),\n",
    "        'cross_modal_alignment': (np.mean(text_to_image_precisions) + np.mean(image_to_text_precisions)) / 2\n",
    "    }\n",
    "\n",
    "# Test clean performance\n",
    "clean_performance = evaluate_multimodal_performance(clip_system)\n",
    "print(\"\\nâœ¨ Clean Multimodal Performance:\")\n",
    "for metric, value in clean_performance.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different corruption scenarios\n",
    "corruption_scenarios = [\n",
    "    {'modality': 'text', 'type': 'gaussian', 'level': 0.1, 'name': 'Text Noise'},\n",
    "    {'modality': 'image', 'type': 'gaussian', 'level': 0.1, 'name': 'Image Noise'},\n",
    "    {'modality': 'text', 'type': 'swap', 'level': 1.0, 'name': 'Text-Image Misalignment'},\n",
    "    {'modality': 'image', 'type': 'zero_out', 'level': 0.3, 'name': 'Image Feature Loss'},\n",
    "    {'modality': 'text', 'type': 'zero_out', 'level': 0.3, 'name': 'Text Feature Loss'}\n",
    "]\n",
    "\n",
    "corruption_results = []\n",
    "\n",
    "for scenario in corruption_scenarios:\n",
    "    print(f\"\\nðŸ§ª Testing {scenario['name']}:\")\n",
    "    \n",
    "    # Create corrupted system\n",
    "    corrupted_system = SimpleCLIPSystem()\n",
    "    \n",
    "    if scenario['modality'] == 'text':\n",
    "        corrupted_text = clip_system.corrupt_modality(\n",
    "            'text', scenario['type'], scenario['level']\n",
    "        )\n",
    "        corrupted_system.add_paired_data(\n",
    "            corrupted_text, clip_system.image_embeddings, \n",
    "            text_desc, image_labels\n",
    "        )\n",
    "    else:\n",
    "        corrupted_image = clip_system.corrupt_modality(\n",
    "            'image', scenario['type'], scenario['level']\n",
    "        )\n",
    "        corrupted_system.add_paired_data(\n",
    "            clip_system.text_embeddings, corrupted_image,\n",
    "            text_desc, image_labels\n",
    "        )\n",
    "    \n",
    "    # Evaluate performance\n",
    "    performance = evaluate_multimodal_performance(corrupted_system)\n",
    "    \n",
    "    result = {\n",
    "        'scenario': scenario['name'],\n",
    "        'modality': scenario['modality'],\n",
    "        'corruption_type': scenario['type'],\n",
    "        **performance\n",
    "    }\n",
    "    corruption_results.append(result)\n",
    "    \n",
    "    print(f\"  Cross-modal alignment: {performance['cross_modal_alignment']:.4f}\")\n",
    "    print(f\"  Textâ†’Image recall: {performance['text_to_image_recall@5']:.4f}\")\n",
    "    print(f\"  Imageâ†’Text recall: {performance['image_to_text_recall@5']:.4f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "corruption_df = pd.DataFrame(corruption_results)\n",
    "print(\"\\nðŸ“Š Collected corruption test results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize: Embedding Space t-SNE Plots\n",
    "\n",
    "Let's visualize how corruption affects the multimodal embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create t-SNE visualizations\n",
    "def plot_multimodal_embeddings(\n",
    "    text_emb: np.ndarray, \n",
    "    image_emb: np.ndarray, \n",
    "    categories: List[str],\n",
    "    title: str,\n",
    "    n_samples_per_category: int = 20\n",
    ") -> None:\n",
    "    \"\"\"Plot t-SNE visualization of multimodal embeddings\"\"\"\n",
    "    \n",
    "    # Sample embeddings for visualization\n",
    "    sampled_text = []\n",
    "    sampled_image = []\n",
    "    labels = []\n",
    "    modalities = []\n",
    "    \n",
    "    samples_per_cat = n_samples_per_category\n",
    "    for i, category in enumerate(categories):\n",
    "        start_idx = i * (len(text_emb) // len(categories))\n",
    "        end_idx = start_idx + samples_per_cat\n",
    "        \n",
    "        sampled_text.extend(text_emb[start_idx:end_idx])\n",
    "        sampled_image.extend(image_emb[start_idx:end_idx])\n",
    "        \n",
    "        labels.extend([category] * samples_per_cat)\n",
    "        labels.extend([category] * samples_per_cat)\n",
    "        modalities.extend(['text'] * samples_per_cat)\n",
    "        modalities.extend(['image'] * samples_per_cat)\n",
    "    \n",
    "    # Combine embeddings\n",
    "    combined_emb = np.vstack([sampled_text, sampled_image])\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    emb_2d = tsne.fit_transform(combined_emb)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot text embeddings\n",
    "    text_mask = np.array(modalities) == 'text'\n",
    "    plt.scatter(emb_2d[text_mask, 0], emb_2d[text_mask, 1], \n",
    "               c=[plt.cm.tab10(i) for i in range(len(categories)) for _ in range(samples_per_cat)],\n",
    "               marker='s', s=100, alpha=0.7, label='Text', edgecolors='black', linewidth=1)\n",
    "    \n",
    "    # Plot image embeddings\n",
    "    image_mask = np.array(modalities) == 'image'\n",
    "    plt.scatter(emb_2d[image_mask, 0], emb_2d[image_mask, 1], \n",
    "               c=[plt.cm.tab10(i) for i in range(len(categories)) for _ in range(samples_per_cat)],\n",
    "               marker='o', s=100, alpha=0.7, label='Image', edgecolors='black', linewidth=1)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='gray', \n",
    "                                  markersize=10, label='Text'),\n",
    "                      plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "                                  markersize=10, label='Image')]\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                         markerfacecolor=plt.cm.tab10(i), markersize=8, \n",
    "                                         label=category.title()))\n",
    "    \n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot clean embeddings\n",
    "plot_multimodal_embeddings(\n",
    "    clip_system.text_embeddings, \n",
    "    clip_system.image_embeddings,\n",
    "    categories,\n",
    "    'Clean Multimodal Embeddings: Text vs Image Alignment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot corrupted embeddings comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Phase 4.2: Multimodal Fusion Breakdown Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Performance degradation by corruption type\n",
    "ax1 = axes[0, 0]\n",
    "corruption_df = corruption_df.sort_values('cross_modal_alignment')\n",
    "bars = ax1.barh(corruption_df['scenario'], corruption_df['cross_modal_alignment'])\n",
    "ax1.axvline(y=clean_performance['cross_modal_alignment'], color='red', linestyle='--', alpha=0.7,\n",
    "            label=f'Clean Baseline ({clean_performance[\"cross_modal_alignment\"]:.4f})')\n",
    "ax1.set_xlabel('Cross-Modal Alignment Score')\n",
    "ax1.set_title('Performance Degradation by Corruption Type')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Modality-specific effects\n",
    "ax2 = axes[0, 1]\n",
    "modality_effects = corruption_df.groupby('modality')['cross_modal_alignment'].mean()\n",
    "bars2 = ax2.bar(['Text Corruption', 'Image Corruption'], modality_effects.values)\n",
    "ax2.axhline(y=clean_performance['cross_modal_alignment'], color='red', linestyle='--', alpha=0.7,\n",
    "            label=f'Clean Baseline ({clean_performance[\"cross_modal_alignment\"]:.4f})')\n",
    "ax2.set_ylabel('Average Alignment Score')\n",
    "ax2.set_title('Modality-Specific Effects')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Similarity matrix heatmap (clean vs corrupted)\n",
    "ax3 = axes[1, 0]\n",
    "# Show a subset of the similarity matrix\n",
    "subset_size = 20\n",
    "clean_sim_subset = clip_system.compute_similarity_matrix()[:subset_size, :subset_size]\n",
    "\n",
    "# Create corrupted version\n",
    "corrupted_text = clip_system.corrupt_modality('text', 'swap', 1.0)\n",
    "corrupted_system = SimpleCLIPSystem()\n",
    "corrupted_system.add_paired_data(corrupted_text, clip_system.image_embeddings, text_desc, image_labels)\n",
    "corrupted_sim_subset = corrupted_system.compute_similarity_matrix()[:subset_size, :subset_size]\n",
    "\n",
    "# Plot difference\n",
    "diff_matrix = corrupted_sim_subset - clean_sim_subset\n",
    "sns.heatmap(diff_matrix, ax=ax3, cmap='RdBu_r', center=0, \n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'label': 'Similarity Change'})\n",
    "ax3.set_title('Similarity Matrix Changes (Corrupted - Clean)')\n",
    "ax3.set_xlabel('Image Index')\n",
    "ax3.set_ylabel('Text Index')\n",
    "\n",
    "# Plot 4: Retrieval success rates\n",
    "ax4 = axes[1, 1]\n",
    "x = np.arange(len(corruption_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, corruption_df['text_to_image_recall@5'], width, \n",
    "                label='Textâ†’Image', alpha=0.8)\n",
    "bars2 = ax4.bar(x + width/2, corruption_df['image_to_text_recall@5'], width,\n",
    "                label='Imageâ†’Text', alpha=0.8)\n",
    "\n",
    "ax4.axhline(y=clean_performance['text_to_image_recall@5'], color='blue', linestyle=':', alpha=0.7)\n",
    "ax4.axhline(y=clean_performance['image_to_text_recall@5'], color='orange', linestyle=':', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Corruption Scenario')\n",
    "ax4.set_ylabel('Recall@5')\n",
    "ax4.set_title('Directional Retrieval Performance')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(corruption_df['scenario'], rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimize: Contrastive Learning Improvements\n",
    "\n",
    "Let's implement techniques to make multimodal systems more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustMultimodalSystem(SimpleCLIPSystem):\n",
    "    \"\"\"Enhanced multimodal system with robustness optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 128, use_contrastive_regularization: bool = False,\n",
    "                 use_modality_adaptation: bool = False, adaptation_strength: float = 0.1):\n",
    "        super().__init__(embedding_dim)\n",
    "        self.use_contrastive_regularization = use_contrastive_regularization\n",
    "        self.use_modality_adaptation = use_modality_adaptation\n",
    "        self.adaptation_strength = adaptation_strength\n",
    "        \n",
    "    def add_paired_data(self, text_embeddings: np.ndarray, image_embeddings: np.ndarray,\n",
    "                       text_descriptions: List[str], image_labels: List[str]):\n",
    "        \"\"\"Add paired data with robustness preprocessing\"\"\"\n",
    "        super().add_paired_data(text_embeddings, image_embeddings, text_descriptions, image_labels)\n",
    "        \n",
    "        if self.use_modality_adaptation:\n",
    "            self._apply_modality_adaptation()\n",
    "            \n",
    "    def _apply_modality_adaptation(self):\n",
    "        \"\"\"Adapt embeddings to be more robust to corruption\"\"\"\n",
    "        # Compute cross-modal centroids\n",
    "        text_centroid = np.mean(self.text_embeddings, axis=0)\n",
    "        image_centroid = np.mean(self.image_embeddings, axis=0)\n",
    "        \n",
    "        # Pull embeddings toward their modality centroids\n",
    "        self.text_embeddings = (1 - self.adaptation_strength) * self.text_embeddings + \\\n",
    "                              self.adaptation_strength * text_centroid\n",
    "        self.image_embeddings = (1 - self.adaptation_strength) * self.image_embeddings + \\\n",
    "                               self.adaptation_strength * image_centroid\n",
    "        \n",
    "        # Renormalize\n",
    "        self.text_embeddings = self.text_embeddings / np.linalg.norm(self.text_embeddings, axis=1, keepdims=True)\n",
    "        self.image_embeddings = self.image_embeddings / np.linalg.norm(self.image_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "    def retrieve_robust(self, query_embedding: np.ndarray, query_modality: str, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Robust retrieval with modality-specific processing\"\"\"\n",
    "        \n",
    "        if query_modality == 'text':\n",
    "            # Text to image retrieval\n",
    "            similarities = cosine_similarity(query_embedding.reshape(1, -1), self.image_embeddings)[0]\n",
    "            \n",
    "            if self.use_contrastive_regularization:\n",
    "                # Boost similarities to images that are well-aligned with text modality\n",
    "                text_centroid = np.mean(self.text_embeddings, axis=0)\n",
    "                alignment_scores = cosine_similarity(self.image_embeddings, text_centroid.reshape(1, -1)).flatten()\n",
    "                similarities = similarities * (1 + 0.1 * alignment_scores)  # Small boost\n",
    "                \n",
    "        else:\n",
    "            # Image to text retrieval\n",
    "            similarities = cosine_similarity(query_embedding.reshape(1, -1), self.text_embeddings)[0]\n",
    "            \n",
    "            if self.use_contrastive_regularization:\n",
    "                # Boost similarities to texts that are well-aligned with image modality\n",
    "                image_centroid = np.mean(self.image_embeddings, axis=0)\n",
    "                alignment_scores = cosine_similarity(self.text_embeddings, image_centroid.reshape(1, -1)).flatten()\n",
    "                similarities = similarities * (1 + 0.1 * alignment_scores)\n",
    "        \n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        return list(zip(top_k_indices, similarities[top_k_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robust multimodal systems\n",
    "robust_configs = [\n",
    "    {'use_contrastive_regularization': False, 'use_modality_adaptation': False, 'name': 'Baseline'},\n",
    "    {'use_contrastive_regularization': True, 'use_modality_adaptation': False, 'name': 'Contrastive Regularization'},\n",
    "    {'use_contrastive_regularization': False, 'use_modality_adaptation': True, 'name': 'Modality Adaptation'},\n",
    "    {'use_contrastive_regularization': True, 'use_modality_adaptation': True, 'name': 'Full Robust'}\n",
    "]\n",
    "\n",
    "robustness_results = []\n",
    "\n",
    "# Test each configuration against different corruption types\n",
    "test_corruptions = ['gaussian', 'swap', 'zero_out']\n",
    "\n",
    "for config in robust_configs:\n",
    "    print(f\"\\nðŸ›¡ï¸ Testing {config['name']}:\")\n",
    "    \n",
    "    for corruption in test_corruptions:\n",
    "        # Create robust system\n",
    "        robust_system = RobustMultimodalSystem(\n",
    "            use_contrastive_regularization=config['use_contrastive_regularization'],\n",
    "            use_modality_adaptation=config['use_modality_adaptation']\n",
    "        )\n",
    "        \n",
    "        # Apply corruption\n",
    "        if corruption == 'gaussian':\n",
    "            corrupted_text = clip_system.corrupt_modality('text', 'gaussian', 0.1)\n",
    "            robust_system.add_paired_data(corrupted_text, clip_system.image_embeddings, text_desc, image_labels)\n",
    "        elif corruption == 'swap':\n",
    "            corrupted_text = clip_system.corrupt_modality('text', 'swap', 1.0)\n",
    "            robust_system.add_paired_data(corrupted_text, clip_system.image_embeddings, text_desc, image_labels)\n",
    "        else:  # zero_out\n",
    "            corrupted_image = clip_system.corrupt_modality('image', 'zero_out', 0.2)\n",
    "            robust_system.add_paired_data(clip_system.text_embeddings, corrupted_image, text_desc, image_labels)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        performance = evaluate_multimodal_performance(robust_system)\n",
    "        \n",
    "        result = {\n",
    "            'config': config['name'],\n",
    "            'corruption': corruption,\n",
    "            **performance\n",
    "        }\n",
    "        robustness_results.append(result)\n",
    "        \n",
    "        print(f\"  {corruption}: alignment={performance['cross_modal_alignment']:.4f}\")\n",
    "\n",
    "# Analyze robustness improvements\n",
    "robust_df = pd.DataFrame(robustness_results)\n",
    "\n",
    "# Calculate improvements over baseline\n",
    "baseline_results = robust_df[robust_df['config'] == 'Baseline'].set_index('corruption')\n",
    "improvement_data = []\n",
    "\n",
    "for config_name in ['Contrastive Regularization', 'Modality Adaptation', 'Full Robust']:\n",
    "    config_results = robust_df[robust_df['config'] == config_name].set_index('corruption')\n",
    "    for corruption in test_corruptions:\n",
    "        baseline_align = baseline_results.loc[corruption, 'cross_modal_alignment']\n",
    "        config_align = config_results.loc[corruption, 'cross_modal_alignment']\n",
    "        improvement = config_align - baseline_align\n",
    "        improvement_pct = (improvement / baseline_align) * 100 if baseline_align > 0 else 0\n",
    "        \n",
    "        improvement_data.append({\n",
    "            'config': config_name,\n",
    "            'corruption': corruption,\n",
    "            'improvement': improvement,\n",
    "            'improvement_pct': improvement_pct\n",
    "        })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_data)\n",
    "print(\"\\nðŸ“ˆ Robustness Improvements:\")\n",
    "print(improvement_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Robustness comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create subplots for each corruption type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Multimodal Robustness: Configuration Comparison by Corruption Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, corruption in enumerate(test_corruptions):\n",
    "    ax = axes[i]\n",
    "    subset = robust_df[robust_df['corruption'] == corruption]\n",
    "    \n",
    "    configs = subset['config']\n",
    "    alignments = subset['cross_modal_alignment']\n",
    "    \n",
    "    bars = ax.bar(configs, alignments, color=['lightcoral', 'skyblue', 'lightgreen', 'gold'])\n",
    "    ax.set_title(f'{corruption.title()} Corruption')\n",
    "    ax.set_ylabel('Cross-Modal Alignment')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Rotate x labels\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, align in zip(bars, alignments):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{align:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **Multimodal alignment is fragile** - Small corruptions in one modality severely impact cross-modal retrieval\n",
    "2. **Misalignment is catastrophic** - Swapping embeddings (perfect misalignment) destroys all alignment\n",
    "3. **Feature loss hurts differently** - Text and image corruption have asymmetric effects\n",
    "4. **Robustness techniques help** - Modality adaptation and contrastive regularization can mitigate some issues\n",
    "\n",
    "### Production Implications:\n",
    "- **Monitor modality quality** - Implement separate health checks for text and image encoders\n",
    "- **Use robust alignment** - Consider techniques like momentum encoders or hard negative mining\n",
    "- **Implement modality-specific processing** - Handle text and image corruption differently\n",
    "- **Regular alignment validation** - Continuously check cross-modal retrieval performance\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with real CLIP models and datasets\n",
    "- Test on actual image-text pairs from the web\n",
    "- Implement production monitoring for multimodal alignment\n",
    "- Explore advanced techniques like cross-modal attention\n",
    "\n",
    "---\n",
    "\n",
    "**Professor's Challenge:** Design a multimodal system that maintains 70% alignment even with 30% of embeddings randomly swapped. What architecture would you use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
