{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.1 Experimental Lab: Retrieval Accuracy Stress Tests\n",
    "\n",
    "**Goal:** Build retrieval system ‚Üí corrupt embeddings with noise ‚Üí measure recall degradation\n",
    "\n",
    "**Break it:** Add adversarial perturbations ‚Üí watch retrieval failures\n",
    "\n",
    "**Visualize:** Precision-recall curves under corruption, error propagation heatmaps\n",
    "\n",
    "**Optimize:** Robust embedding normalization, outlier detection filters\n",
    "\n",
    "---\n",
    "\n",
    "## Professor's Notes\n",
    "\n",
    "St. Mark, this lab teaches you that retrieval systems are fragile. In production, embeddings get corrupted by:\n",
    "- Numerical precision issues\n",
    "- Memory corruption\n",
    "- Adversarial attacks\n",
    "- Data drift\n",
    "\n",
    "Understanding how retrieval degrades under stress helps you build robust systems that don't fail silently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üî¨ Phase 4.1: Retrieval Accuracy Stress Tests\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Clean Retrieval System\n",
    "\n",
    "First, let's create a simple but effective retrieval system using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRetrievalSystem:\n",
    "    \"\"\"A simple embedding-based retrieval system\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 128):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def add_documents(self, docs: List[str], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents with their embeddings\"\"\"\n",
    "        self.documents = docs\n",
    "        self.embeddings = embeddings.copy()\n",
    "        print(f\"‚úÖ Added {len(docs)} documents with {embeddings.shape[1]}D embeddings\")\n",
    "        \n",
    "    def retrieve(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Retrieve top-k most similar documents\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "            \n",
    "        # Compute similarities\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k indices and scores\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_k_scores = similarities[top_k_indices]\n",
    "        \n",
    "        return list(zip(top_k_indices, top_k_scores))\n",
    "    \n",
    "    def corrupt_embeddings(self, noise_level: float, corruption_type: str = 'gaussian') -> np.ndarray:\n",
    "        \"\"\"Corrupt embeddings with different types of noise\"\"\"\n",
    "        corrupted = self.embeddings.copy()\n",
    "        \n",
    "        if corruption_type == 'gaussian':\n",
    "            # Add Gaussian noise\n",
    "            noise = np.random.normal(0, noise_level, corrupted.shape)\n",
    "            corrupted += noise\n",
    "            \n",
    "        elif corruption_type == 'salt_pepper':\n",
    "            # Salt and pepper noise (random spikes)\n",
    "            mask = np.random.random(corrupted.shape) < noise_level\n",
    "            corrupted[mask] = np.random.choice([-1, 1], size=mask.sum())\n",
    "            \n",
    "        elif corruption_type == 'adversarial':\n",
    "            # Adversarial perturbations (small but targeted)\n",
    "            directions = np.random.normal(0, 1, corrupted.shape)\n",
    "            directions = directions / np.linalg.norm(directions, axis=1, keepdims=True)\n",
    "            corrupted += noise_level * directions\n",
    "            \n",
    "        # Renormalize to maintain unit length (important for cosine similarity)\n",
    "        norms = np.linalg.norm(corrupted, axis=1, keepdims=True)\n",
    "        corrupted = corrupted / norms\n",
    "        \n",
    "        return corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for testing\n",
    "def create_test_data(n_docs: int = 1000, n_queries: int = 100, embedding_dim: int = 128):\n",
    "    \"\"\"Create synthetic documents and queries with embeddings\"\"\"\n",
    "    \n",
    "    # Generate document embeddings (clustered)\n",
    "    doc_centers = np.random.normal(0, 1, (10, embedding_dim))  # 10 clusters\n",
    "    doc_embeddings, doc_labels = make_blobs(\n",
    "        n_samples=n_docs, \n",
    "        centers=doc_centers,\n",
    "        cluster_std=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    doc_embeddings = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Create document texts\n",
    "    documents = [f\"Document {i} in cluster {label}\" for i, label in enumerate(doc_labels)]\n",
    "    \n",
    "    # Generate query embeddings (some matching docs, some outliers)\n",
    "    query_centers = np.vstack([doc_centers[:8], np.random.normal(0, 2, (2, embedding_dim))])\n",
    "    query_embeddings, query_labels = make_blobs(\n",
    "        n_samples=n_queries,\n",
    "        centers=query_centers,\n",
    "        cluster_std=0.2,\n",
    "        random_state=43\n",
    "    )\n",
    "    \n",
    "    # Normalize query embeddings\n",
    "    query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    return documents, doc_embeddings, doc_labels, query_embeddings, query_labels\n",
    "\n",
    "# Create test data\n",
    "documents, doc_embeddings, doc_labels, query_embeddings, query_labels = create_test_data()\n",
    "\n",
    "print(f\"üìÑ Created {len(documents)} documents\")\n",
    "print(f\"üîç Created {len(query_embeddings)} queries\")\n",
    "print(f\"üìä Document clusters: {len(np.unique(doc_labels))}\")\n",
    "print(f\"üéØ Query clusters: {len(np.unique(query_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrieval system\n",
    "retrieval_system = SimpleRetrievalSystem()\n",
    "retrieval_system.add_documents(documents, doc_embeddings)\n",
    "\n",
    "# Test clean retrieval\n",
    "test_query = query_embeddings[0:1]  # First query\n",
    "results = retrieval_system.retrieve(test_query, k=5)\n",
    "\n",
    "print(\"\\nüîç Clean Retrieval Results:\")\n",
    "print(\"Query cluster:\", query_labels[0])\n",
    "for rank, (doc_idx, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. Doc {doc_idx} (Cluster {doc_labels[doc_idx]}): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Break It: Add Corruption\n",
    "\n",
    "Now let's systematically corrupt the embeddings and see how retrieval performance degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_performance(\n",
    "    system: SimpleRetrievalSystem, \n",
    "    queries: np.ndarray, \n",
    "    query_labels: np.ndarray,\n",
    "    doc_labels: np.ndarray,\n",
    "    k: int = 5\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval performance metrics\"\"\"\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for query_emb, query_label in zip(queries, query_labels):\n",
    "        results = system.retrieve(query_emb.reshape(1, -1), k=k)\n",
    "        retrieved_labels = [doc_labels[doc_idx] for doc_idx, _ in results]\n",
    "        \n",
    "        # Calculate precision@K (fraction of retrieved docs in same cluster)\n",
    "        relevant_retrieved = sum(1 for label in retrieved_labels if label == query_label)\n",
    "        precision = relevant_retrieved / k\n",
    "        precisions.append(precision)\n",
    "        \n",
    "        # Calculate recall@K (fraction of relevant docs retrieved)\n",
    "        total_relevant = sum(1 for label in doc_labels if label == query_label)\n",
    "        recall = relevant_retrieved / min(total_relevant, k)  # Cap at k\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return {\n",
    "        'precision@k': np.mean(precisions),\n",
    "        'recall@k': np.mean(recalls),\n",
    "        'f1@k': 2 * np.mean(precisions) * np.mean(recalls) / (np.mean(precisions) + np.mean(recalls))\n",
    "    }\n",
    "\n",
    "# Test clean performance\n",
    "clean_performance = evaluate_retrieval_performance(\n",
    "    retrieval_system, query_embeddings, query_labels, doc_labels, k=5\n",
    ")\n",
    "\n",
    "print(\"\\n‚ú® Clean System Performance:\")\n",
    "for metric, value in clean_performance.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different corruption levels and types\n",
    "corruption_levels = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "corruption_types = ['gaussian', 'salt_pepper', 'adversarial']\n",
    "\n",
    "corruption_results = []\n",
    "\n",
    "for corruption_type in corruption_types:\n",
    "    print(f\"\\nüß™ Testing {corruption_type.upper()} corruption:\")\n",
    "    \n",
    "    for noise_level in corruption_levels:\n",
    "        # Create corrupted system\n",
    "        corrupted_embeddings = retrieval_system.corrupt_embeddings(noise_level, corruption_type)\n",
    "        corrupted_system = SimpleRetrievalSystem()\n",
    "        corrupted_system.add_documents(documents, corrupted_embeddings)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        performance = evaluate_retrieval_performance(\n",
    "            corrupted_system, query_embeddings, query_labels, doc_labels, k=5\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'corruption_type': corruption_type,\n",
    "            'noise_level': noise_level,\n",
    "            **performance\n",
    "        }\n",
    "        corruption_results.append(result)\n",
    "        \n",
    "        print(f\"  Noise {noise_level:.3f}: P@5={performance['precision@k']:.4f}, R@5={performance['recall@k']:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(corruption_results)\n",
    "print(\"\\nüìä Collected\", len(results_df), \"experimental results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize: Precision-Recall Curves Under Corruption\n",
    "\n",
    "Let's create comprehensive visualizations to understand how retrieval degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Phase 4.1: Retrieval Accuracy Stress Tests', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Precision vs Noise Level\n",
    "ax1 = axes[0, 0]\n",
    "for corruption_type in corruption_types:\n",
    "    subset = results_df[results_df['corruption_type'] == corruption_type]\n",
    "    ax1.plot(subset['noise_level'], subset['precision@k'], \n",
    "             marker='o', linewidth=2, label=corruption_type.title())\n",
    "\n",
    "ax1.axhline(y=clean_performance['precision@k'], color='red', linestyle='--', alpha=0.7,\n",
    "            label=f'Clean Baseline ({clean_performance[\"precision@k\"]:.4f})')\n",
    "ax1.set_xlabel('Noise Level')\n",
    "ax1.set_ylabel('Precision@5')\n",
    "ax1.set_title('Retrieval Precision Degradation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Recall vs Noise Level\n",
    "ax2 = axes[0, 1]\n",
    "for corruption_type in corruption_types:\n",
    "    subset = results_df[results_df['corruption_type'] == corruption_type]\n",
    "    ax2.plot(subset['noise_level'], subset['recall@k'], \n",
    "             marker='s', linewidth=2, label=corruption_type.title())\n",
    "\n",
    "ax2.axhline(y=clean_performance['recall@k'], color='red', linestyle='--', alpha=0.7,\n",
    "            label=f'Clean Baseline ({clean_performance[\"recall@k\"]:.4f})')\n",
    "ax2.set_xlabel('Noise Level')\n",
    "ax2.set_ylabel('Recall@5')\n",
    "ax2.set_title('Retrieval Recall Degradation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: F1 Score Degradation\n",
    "ax3 = axes[1, 0]\n",
    "for corruption_type in corruption_types:\n",
    "    subset = results_df[results_df['corruption_type'] == corruption_type]\n",
    "    ax3.plot(subset['noise_level'], subset['f1@k'], \n",
    "             marker='^', linewidth=2, label=corruption_type.title())\n",
    "\n",
    "ax3.axhline(y=clean_performance['f1@k'], color='red', linestyle='--', alpha=0.7,\n",
    "            label=f'Clean Baseline ({clean_performance[\"f1@k\"]:.4f})')\n",
    "ax3.set_xlabel('Noise Level')\n",
    "ax3.set_ylabel('F1@5')\n",
    "ax3.set_title('Overall Retrieval Performance (F1)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Performance Drop Comparison\n",
    "ax4 = axes[1, 1]\n",
    "corruption_at_01 = results_df[results_df['noise_level'] == 0.1]\n",
    "performance_drop = []\n",
    "\n",
    "for _, row in corruption_at_01.iterrows():\n",
    "    drop = clean_performance['f1@k'] - row['f1@k']\n",
    "    performance_drop.append({\n",
    "        'type': row['corruption_type'],\n",
    "        'drop': drop,\n",
    "        'percentage': (drop / clean_performance['f1@k']) * 100\n",
    "    })\n",
    "\n",
    "drop_df = pd.DataFrame(performance_drop)\n",
    "bars = ax4.bar(drop_df['type'], drop_df['percentage'], \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "\n",
    "ax4.set_xlabel('Corruption Type')\n",
    "ax4.set_ylabel('Performance Drop (%)')\n",
    "ax4.set_title('F1 Score Drop at 10% Noise Level')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, pct in zip(bars, drop_df['percentage']):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error propagation heatmap\n",
    "def create_error_propagation_heatmap():\n",
    "    \"\"\"Visualize how errors propagate through the retrieval system\"\"\"\n",
    "    \n",
    "    # Test with a specific query and show how corruption affects ranking\n",
    "    test_query_idx = 0\n",
    "    test_query_emb = query_embeddings[test_query_idx:test_query_idx+1]\n",
    "    true_cluster = query_labels[test_query_idx]\n",
    "    \n",
    "    # Get rankings for different corruption levels\n",
    "    ranking_changes = []\n",
    "    \n",
    "    for noise_level in [0.0, 0.05, 0.1, 0.2]:\n",
    "        corrupted_emb = retrieval_system.corrupt_embeddings(noise_level, 'gaussian')\n",
    "        corrupted_system = SimpleRetrievalSystem()\n",
    "        corrupted_system.add_documents(documents, corrupted_emb)\n",
    "        \n",
    "        results = corrupted_system.retrieve(test_query_emb, k=10)\n",
    "        rankings = [(doc_idx, score, doc_labels[doc_idx] == true_cluster) for doc_idx, score in results]\n",
    "        ranking_changes.append((noise_level, rankings))\n",
    "    \n",
    "    # Create heatmap data\n",
    "    n_docs = 20  # Show top 20 docs\n",
    "    heatmap_data = np.zeros((len(ranking_changes), n_docs))\n",
    "    \n",
    "    for i, (noise_level, rankings) in enumerate(ranking_changes):\n",
    "        for j, (doc_idx, score, is_relevant) in enumerate(rankings[:n_docs]):\n",
    "            heatmap_data[i, j] = 1 if is_relevant else 0\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.heatmap(heatmap_data, \n",
    "                xticklabels=[f'Doc {i+1}' for i in range(n_docs)],\n",
    "                yticklabels=[f'Noise {level:.3f}' for level, _ in ranking_changes],\n",
    "                cmap='RdYlGn', cbar_kws={'label': 'Relevant to Query'})\n",
    "    \n",
    "    plt.title('Error Propagation: How Corruption Affects Document Ranking')\n",
    "    plt.xlabel('Retrieved Document Position')\n",
    "    plt.ylabel('Corruption Level')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ranking_changes\n",
    "\n",
    "print(\"\\nüî• Error Propagation Analysis:\")\n",
    "ranking_analysis = create_error_propagation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimize: Robust Retrieval Techniques\n",
    "\n",
    "Now let's implement and test optimization techniques to make retrieval more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustRetrievalSystem(SimpleRetrievalSystem):\n",
    "    \"\"\"Enhanced retrieval system with robustness optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 128, use_normalization: bool = True, \n",
    "                 use_outlier_filter: bool = False, outlier_threshold: float = 2.0):\n",
    "        super().__init__(embedding_dim)\n",
    "        self.use_normalization = use_normalization\n",
    "        self.use_outlier_filter = use_outlier_filter\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        \n",
    "    def add_documents(self, docs: List[str], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents with robustness preprocessing\"\"\"\n",
    "        super().add_documents(docs, embeddings)\n",
    "        \n",
    "        if self.use_normalization:\n",
    "            # Apply robust normalization\n",
    "            norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "            # Avoid division by very small numbers\n",
    "            norms = np.maximum(norms, 1e-8)\n",
    "            self.embeddings = self.embeddings / norms\n",
    "            \n",
    "        if self.use_outlier_filter:\n",
    "            # Remove outlier embeddings\n",
    "            centroid = np.mean(self.embeddings, axis=0)\n",
    "            distances = np.linalg.norm(self.embeddings - centroid, axis=1)\n",
    "            threshold = np.mean(distances) + self.outlier_threshold * np.std(distances)\n",
    "            keep_mask = distances <= threshold\n",
    "            \n",
    "            self.documents = [doc for doc, keep in zip(self.documents, keep_mask) if keep]\n",
    "            self.embeddings = self.embeddings[keep_mask]\n",
    "            \n",
    "            print(f\"üßπ Filtered {len(keep_mask) - keep_mask.sum()} outlier documents\")\n",
    "    \n",
    "    def retrieve_robust(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Robust retrieval with confidence scoring\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "            \n",
    "        # Get more candidates than needed\n",
    "        candidates = self.retrieve(query_embedding, k=k*3)\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Apply confidence filtering\n",
    "        scores = np.array([score for _, score in candidates])\n",
    "        \n",
    "        # Filter out low-confidence results\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        confidence_threshold = mean_score - 0.5 * std_score\n",
    "        \n",
    "        filtered_candidates = [\n",
    "            (idx, score) for (idx, score) in candidates \n",
    "            if score >= confidence_threshold\n",
    "        ]\n",
    "        \n",
    "        return filtered_candidates[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robust retrieval systems\n",
    "robust_configs = [\n",
    "    {'use_normalization': True, 'use_outlier_filter': False, 'name': 'Normalized Only'},\n",
    "    {'use_normalization': True, 'use_outlier_filter': True, 'name': 'Normalized + Outlier Filter'},\n",
    "    {'use_normalization': False, 'use_outlier_filter': False, 'name': 'Baseline'}\n",
    "]\n",
    "\n",
    "robustness_results = []\n",
    "\n",
    "for config in robust_configs:\n",
    "    print(f\"\\nüõ°Ô∏è Testing {config['name']}:\")\n",
    "    \n",
    "    # Test on corrupted data\n",
    "    for noise_level in [0.0, 0.1, 0.2]:\n",
    "        # Create corrupted embeddings\n",
    "        corrupted_emb = retrieval_system.corrupt_embeddings(noise_level, 'gaussian')\n",
    "        \n",
    "        # Create robust system\n",
    "        robust_system = RobustRetrievalSystem(\n",
    "            use_normalization=config['use_normalization'],\n",
    "            use_outlier_filter=config['use_outlier_filter']\n",
    "        )\n",
    "        robust_system.add_documents(documents, corrupted_emb)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        performance = evaluate_retrieval_performance(\n",
    "            robust_system, query_embeddings, query_labels, doc_labels, k=5\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'config': config['name'],\n",
    "            'noise_level': noise_level,\n",
    "            **performance\n",
    "        }\n",
    "        robustness_results.append(result)\n",
    "        \n",
    "        print(f\"  Noise {noise_level:.1f}: F1={performance['f1@k']:.4f}\")\n",
    "\n",
    "# Compare robustness improvements\n",
    "robust_df = pd.DataFrame(robustness_results)\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "baseline_results = robust_df[robust_df['config'] == 'Baseline'].set_index('noise_level')\n",
    "improvement_data = []\n",
    "\n",
    "for config_name in ['Normalized Only', 'Normalized + Outlier Filter']:\n",
    "    config_results = robust_df[robust_df['config'] == config_name].set_index('noise_level')\n",
    "    for noise_level in [0.1, 0.2]:\n",
    "        baseline_f1 = baseline_results.loc[noise_level, 'f1@k']\n",
    "        config_f1 = config_results.loc[noise_level, 'f1@k']\n",
    "        improvement = config_f1 - baseline_f1\n",
    "        improvement_pct = (improvement / baseline_f1) * 100\n",
    "        \n",
    "        improvement_data.append({\n",
    "            'config': config_name,\n",
    "            'noise_level': noise_level,\n",
    "            'improvement': improvement,\n",
    "            'improvement_pct': improvement_pct\n",
    "        })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_data)\n",
    "print(\"\\nüìà Robustness Improvements:\")\n",
    "print(improvement_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Robustness comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot F1 scores for different configurations\n",
    "for config_name in robust_df['config'].unique():\n",
    "    subset = robust_df[robust_df['config'] == config_name]\n",
    "    plt.plot(subset['noise_level'], subset['f1@k'], \n",
    "             marker='o', linewidth=3, label=config_name, markersize=8)\n",
    "\n",
    "plt.xlabel('Noise Level', fontsize=12)\n",
    "plt.ylabel('F1@5 Score', fontsize=12)\n",
    "plt.title('Retrieval Robustness: Configuration Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key improvements\n",
    "for _, row in improvement_df.iterrows():\n",
    "    if row['noise_level'] == 0.2:\n",
    "        plt.annotate(f\"+{row['improvement_pct']:.1f}%\", \n",
    "                    xy=(row['noise_level'], \n",
    "                        robust_df[(robust_df['config'] == row['config']) & \n",
    "                                 (robust_df['noise_level'] == row['noise_level'])]['f1@k'].values[0]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **Retrieval systems are fragile** - Small amounts of noise can cause significant performance degradation\n",
    "2. **Different corruption types matter** - Adversarial perturbations are more damaging than random noise\n",
    "3. **Robustness techniques help** - Normalization and outlier filtering can mitigate some degradation\n",
    "4. **Error propagation is real** - Corruption doesn't just reduce scores, it changes ranking order\n",
    "\n",
    "### Production Implications:\n",
    "- **Monitor embedding quality** - Implement checks for embedding corruption\n",
    "- **Use robust similarity measures** - Consider alternatives to pure cosine similarity\n",
    "- **Implement fallback strategies** - Have backup retrieval methods for when embeddings fail\n",
    "- **Regular retraining** - Embeddings drift over time and need periodic refreshing\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different embedding models (sentence transformers, etc.)\n",
    "- Test on real datasets with actual semantic similarity\n",
    "- Implement production monitoring for embedding health\n",
    "\n",
    "---\n",
    "\n",
    "**Professor's Challenge:** Can you implement a retrieval system that maintains 90% of its clean performance even with 20% noise? What techniques would you use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
