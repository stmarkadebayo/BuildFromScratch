{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "**Welcome, Mark!** In this foundational notebook, we'll build linear regression from first principles. Think of this as learning to balance a broomstick: understanding the mathematical pendulum before engineering the automation.\n",
    "\n",
    "We'll explore:\n",
    "1. **Ordinary Least Squares (OLS)** - The \"perfect\" closed-form solution\n",
    "2. **Gradient Descent Variants** - SGD, Mini-batch, Batch - each with different computational trade-offs\n",
    "\n",
    "By the end, you'll understand why optimization algorithms are the engine of machine learning.\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Linear regression is our first optimization problem: finding the best line through data points. In coding terms:\n",
    "- **Features (X)**: Input variables, like clinical measurements\n",
    "- **Weights (Î¸)**: Learned parameters that scale each feature's importance  \n",
    "- **MSC (Loss)**: Measures how wrong our predictions are\n",
    "\n",
    "**Key Question:** How do we automatically find the \"best\" weights without human guesswork?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Setting Up Our Experiment\n",
    "\n",
    "Before diving into math, we need data. We'll create synthetic data using scikit-learn's `make_regression`.\n",
    "\n",
    "This is like collecting samples in a lab - we need consistent, controlled conditions to test our hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy: Our matrix mathematics library - handles arrays like Excel but much faster\n",
    "import matplotlib.pyplot as plt  # Matplotlib: For creating visualizations - turning numbers into insights\n",
    "from sklearn.linear_model import LinearRegression  # Scikit-learn: Our comparison baseline - established methods\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Metrics: To evaluate our model's performance\n",
    "from sklearn.datasets import make_regression  # Data generator: Creates synthetic datasets for testing\n",
    "\n",
    "# Create synthetic data mimicking real-world relationships (like patient features â†’ disease risk)\n",
    "# n_samples: Number of data points (patients)\n",
    "# n_features: Number of input variables (clinical measurements) \n",
    "# noise: Random variation, just like measurement error in labs\n",
    "X, y = make_regression(n_samples=1000,    # 1000 patients/data points\n",
    "                       n_features=5,      # 5 clinical measurements each\n",
    "                       noise=10,           # Some real-world noise/uncertainty\n",
    "                       random_state=42)    # Reproducible results - science demands repeatability\n",
    "\n",
    "# Reshape y to column vector for matrix operations (NumPy convention)\n",
    "y = y.reshape(-1, 1)  # Changes y from 1D array (n,) to 2D array (n, 1)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")  # X: (samples, features), y: (samples, outputs)\n",
    "\n",
    "# Add \"bias\" or \"intercept\" term\n",
    "# Why? Biology example: Even with zero features, there might be baseline disease risk\n",
    "# Mathematically: This accounts for the y-intercept in our linear equation y = mx + b\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Concatenate column of ones to feature matrix\n",
    "print(f\"Dataset with intercept: {X_b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** We've just prepared our dataset.\n",
    "\n",
    "- **Key Concept:** The intercept term (column of 1's) allows our model to fit a line that doesn't necessarily pass through origin. In healthcare terms: \"baseline risk even when all measured factors are zero.\"\n",
    "- **Matrix Shapes:** Understanding dimensions is crucial. X_b is now (1000 samples Ã— 6 features) = 6 parameters to learn.\n",
    "- **Why Synthetic Data?** Allows controlled experimentation - we know the \"ground truth\" to validate our algorithms.\n",
    "\n",
    "**Reflection Question:** If we removed the intercept term, how would that affect predictions for Nigerian healthcare data where baseline conditions vary by region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Ordinary Least Squares (OLS) - The Mathematical \"Perfect\" Solution\n",
    "\n",
    "OLS uses linear algebra to find the optimal parameters in one calculation.\n",
    "\n",
    "**Why it's \"perfect\":**\n",
    "- Computes exact minimum without iterative guessing\n",
    "- Mathematically derived from calculus (setting derivatives to zero)\n",
    "\n",
    "**The math:** Î¸ = (X^T X)^(-1) X^T y\n",
    "\n",
    "This is the normal equation - like solving simultaneous equations in middle school, but for hundreds of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_closed_form(X, y):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X: Feature matrix (m samples Ã— n+1 features, includes intercept)\n",
    "    y: Target vector (m Ã— 1)\n",
    "    \n",
    "    Returns:\n",
    "    theta: Optimal parameters (n+1 Ã— 1), or None if matrix singular\n",
    "    \n",
    "    Mathematical derivation:\n",
    "    We minimize J(Î¸) = (1/m) * Î£ ||XÂ·Î¸ - y||Â²\n",
    "    Derivative becomes: X^T X Î¸ - X^T y = 0\n",
    "    Solving gives: Î¸ = (X^T X)^(-1) X^T y\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Compute X^T X (gram matrix)\n",
    "        # This creates a (nÃ—n) matrix representing feature correlations\n",
    "        XTX = X.T @ X  \n",
    "        \n",
    "        # Step 2: Compute matrix inverse\n",
    "        # Only possible if features are linearly independent (no exact duplicates)\n",
    "        XTX_inv = np.linalg.inv(XTX)\n",
    "        \n",
    "        # Step 3: Compute X^T y (projection of target onto features)\n",
    "        XTY = X.T @ y  \n",
    "        \n",
    "        # Step 4: Solve for theta using normal equations\n",
    "        theta = XTX_inv @ XTY  \n",
    "        \n",
    "        return theta\n",
    "        \n",
    "    except np.linalg.LinAlgError:\n",
    "        # Matrix inversion fails when features are perfectly correlated\n",
    "        # (multicollinearity) - like trying to solve an underdetermined system\n",
    "        print(\"Matrix is singular - features are linearly dependent\")\n",
    "        return None\n",
    "\n",
    "# Compute optimal parameters using algebra (no iteration needed!)\n",
    "theta_ols = ols_closed_form(X_b, y)\n",
    "if theta_ols is not None:\n",
    "    print(f\"OLS Parameters: {theta_ols.flatten()[:3]}...\")  # Show first 3 for brevity\n",
    "    print(\"âœ“ Exact solution found - no approximation involved!\"\n",
    "else:\n",
    "    print(\"OLS failed - multicollinearity detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** \n",
    "\n",
    "**What this code does:** Implements the normal equations\n",
    "\n",
    "**Why it's important:** \n",
    "- **Exact vs Approximate:** OLS gives mathematically perfect solution vs iterative optimization methods\n",
    "- **No hyperparameters:** Learning rate isn't needed\n",
    "- **Computational complexity:** O(nÂ³) matrix inversion becomes expensive for n > 10,000 features\n",
    "\n",
    "**Healthcare Analogy:** This is like having unlimited funding for perfect diagnostic equipment. But in practice (limited budget/time), we use faster approximate methods.\n",
    "\n",
    "**Reflection Question:** When might OLS fail in real-world Nigerian healthcare data? (Hint: Think about correlated symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Gradient Descent - The \"Iterative Climbing\" Algorithm\n",
    "\n",
    "Now we learn optimization through iteration - like hill climbing but downwards.\n",
    "\n",
    "**The algorithm:**\n",
    "1. Start with random weights\n",
    "2. Compute gradients (direction of steepest ascent)\n",
    "3. Take small step downhill (opposite of gradient)\n",
    "4. Repeat until we reach the bottom (minimum)\n",
    "\n",
    "**Learning rate (Î·):** Controls step size - too small = slow convergence, too large = overshoot minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gradients(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute partial derivatives of MSE with respect to each parameter.\n",
    "    \n",
    "    Mathematical derivation:\n",
    "    J(Î¸) = (1/m) Î£ (prediction - actual)Â²\n",
    "    âˆ‚J/âˆ‚Î¸â±¼ = (2/m) Î£ (prediction - actual) Ã— xâ±¼^i\n",
    "    \n",
    "    In vector form: âˆ‡J = (2/m) X^T (XÎ¸ - y)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  # Number of training samples\n",
    "    \n",
    "    # Step 1: Make predictions using current parameters\n",
    "    predictions = X @ theta  # Matrix multiplication: each sample gets its own prediction\n",
    "    \n",
    "    # Step 2: Calculate prediction errors\n",
    "    errors = predictions - y  # Vector of (prediction - actual) for each sample\n",
    "    \n",
    "    # Step 3: Compute gradients for each parameter\n",
    "    # Outer product X^T @ errors distributes errors back to features\n",
    "    # Factor of 2/m comes from derivative of squared loss\n",
    "    gradients = (2/m) * X.T @ errors  # Average of individual sample gradients\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Batch Gradient Descent: Uses ALL training samples for each parameter update.\n",
    "    \n",
    "    Pros: Stable, exact gradients, easy to parallelize\n",
    "    Cons: Slow for large datasets (can't fit in memory)\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # m: samples, n: features (including intercept)\n",
    "    \n",
    "    # Initialize parameters randomly (avoids getting stuck in local minima)\n",
    "    theta = np.random.randn(n, 1)  # Random weights, shape matches feature count\n",
    "    \n",
    "    losses = []  # Track loss history for visualization\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Step 1: Compute direction of steepest ascent (gradients)\n",
    "        gradients = mean_squared_error_gradients(X, y, theta)\n",
    "        \n",
    "        # Step 2: Update parameters (take step downhill)\n",
    "        # Negative gradient because we want to minimize loss\n",
    "        theta -= learning_rate * gradients  \n",
    "        \n",
    "        # Step 3: Track progress\n",
    "        predictions = X @ theta\n",
    "        loss = np.mean((predictions - y) ** 2)  # Current loss value\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Step 4: Convergence check (early stopping)\n",
    "        if iteration > 0 and abs(losses[-1] - losses[-2]) < threshold:\n",
    "            print(f\"Converged after {iteration} iterations - changing very slowly\")\n",
    "            break\n",
    "            \n",
    "    return theta, losses\n",
    "\n",
    "# Apply BGD to our dataset\n",
    "theta_bgd, losses_bgd = batch_gradient_descent(X_b, y, \n",
    "                                               learning_rate=0.1,  # Aggressive learning for quick convergence\n",
    "                                               n_iterations=1000)\n",
    "print(f\"BGD Parameters: {theta_bgd.flatten()[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** \n",
    "\n",
    "**Gradient Computation (mean_squared_error_gradients):**\n",
    "- **Key insight:** Gradients tell us how much each parameter is \"contributing\" to the overall error\n",
    "- **Vectorization:** Using matrix operations for speed on all samples simultaneously\n",
    "- **Partial derivatives:** Shows how J changes with tiny changes in each Î¸\n",
    "\n",
    "**Batch GD Process:**\n",
    "- **Why \"batch\"?** Uses entire dataset for each gradient computation\n",
    "- **Stability:** Less noise than single-sample updates\n",
    "- **Convergence check:** Monitors loss improvement to avoid wasting computation\n",
    "\n",
    "**Reflection Questions:** \n",
    "1. How would using larger learning rates affect the training process?\n",
    "2. In Nigerian healthcare applications, when might batch GD be preferred over single-patient updates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Mini-batch Gradient Descent - The Practical Compromise\n",
    "\n",
    "**Self-test: Why not always use Batch GD or OLS?** Answer: Computational limits! As datasets grow (millions of hospitals records), we need compromise methods.\n",
    "\n",
    "**Mini-batch approach:** Split data into small chunks (batches) and update parameters per batch.\n",
    "\n",
    "Benefits: Faster than batch, less noisy than individual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, n_epochs=50):\n",
    "    \"\"\"\n",
    "    Mini-batch Gradient Descent: Compromise between Batch GD and Stochastic GD.\n",
    "    \n",
    "    Parameters:\n",
    "    batch_size: Samples per parameter update (higher = more stable, slower)\n",
    "    n_epochs: Complete passes through training data\n",
    "    \n",
    "    Trade-offs:\n",
    "    - Small batches: Faster updates, more noisy gradients  \n",
    "    - Large batches: Smoother gradients, more computation\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # m: total samples, n: features\n",
    "    \n",
    "    theta = np.random.randn(n, 1)  # Initialize parameters\n",
    "    losses = []  # Track loss over epochs\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data each epoch (breaks correlation between mini-batches)\n",
    "        # Like shuffling a deck before dealing - removes ordering bias\n",
    "        shuffled_indices = np.random.permutation(m)  # Random permutation of sample indices\n",
    "        X_shuffled = X[shuffled_indices]  # Reorder rows according to shuffle\n",
    "        y_shuffled = y[shuffled_indices]  # Corresponding target reordering\n",
    "        \n",
    "        # Process mini-batches within this epoch\n",
    "        for start_idx in range(0, m, batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            # Extract current mini-batch\n",
    "            xi = X_shuffled[start_idx:end_idx]  # Subset of features\n",
    "            yi = y_shuffled[start_idx:end_idx]  # Corresponding targets\n",
    "            \n",
    "            # Compute gradient on this mini-batch only\n",
    "            gradients = mean_squared_error_gradients(xi, yi, theta)\n",
    "            \n",
    "            # Update parameters using mini-batch gradient\n",
    "            theta -= learning_rate * gradients\n",
    "            \n",
    "        # Checkpoint: Calculate loss on FULL dataset\n",
    "        predictions = X @ theta\n",
    "        loss = np.mean((predictions - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Optional progress reporting\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "            \n",
    "    return theta, losses\n",
    "\n",
    "# Apply Mini-batch GD\n",
    "theta_mbgd, losses_mbgd = mini_batch_gradient_descent(X_b, y, \n",
    "                                                     learning_rate=0.1,\n",
    "                                                     batch_size=32,  # 32 samples per update\n",
    "                                                     n_epochs=50)    # 50 passes through data\n",
    "print(f\"Mini-batch GD Parameters: {theta_mbgd.flatten()[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Mini-batch Strategy:**\n",
    "- **Shuffling:** Prevents learning order-dependent patterns\n",
    "- **Batch processing:** Updates parameters every 32 samples instead of 1000\n",
    "- **Per-epoch checkpointing:** Evaluates full dataset performance\n",
    "\n",
    "**Why this matters:** In healthcare AI:\n",
    "- Hospitals might process patient batches rather than one-by-one\n",
    "- Balances training speed with stability\n",
    "- Prevents overfitting to specific patient groups\n",
    "\n",
    "**Reflection Question:** How would batch_size affect training time and model generalization for Nigerian health data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Stochastic Gradient Descent - The \"Online Learning\" Approach\n",
    "\n",
    "**Stochastic** means \"random\" - we update parameters after each individual sample!\n",
    "\n",
    "**Advantages:** \n",
    "- Very fast parameter updates\n",
    "- Can process data as it arrives (streaming)\n",
    "\n",
    "**Disadvantages:**\n",
    "- Very noisy gradients\n",
    "- Final result is approximate\n",
    "\n",
    "**Real-world use:** Updating recommendation systems as new users interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=50):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent: Update after EVERY single sample.\n",
    "    \n",
    "    Parameters:\n",
    "    learning_rate: Must be very small to prevent overshooting due to noise\n",
    "    n_epochs: Number of complete data passes (multiple exposures)\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # m: total samples, n: features\n",
    "    \n",
    "    theta = np.random.randn(n, 1)  # Random initialization\n",
    "    losses = []  # Track loss per epoch\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle samples for each epoch\n",
    "        sample_order = np.random.permutation(m)\n",
    "        \n",
    "        # Process each sample individually\n",
    "        for sample_idx in sample_order:\n",
    "            # Extract single sample\n",
    "            xi = X[sample_idx:sample_idx+1]  # Keep as 2D: (1, features)\n",
    "            yi = y[sample_idx:sample_idx+1]  # Keep as 2D: (1, outputs)\n",
    "            \n",
    "            # Compute gradient based on this SINGLE sample\n",
    "            # Very noisy! One patient's measurements can drastically change directions\n",
    "            gradients = mean_squared_error_gradients(xi, yi, theta)\n",
    "            \n",
    "            # Update parameters immediately\n",
    "            # Small learning rate critical here due to noise\n",
    "            theta -= learning_rate * gradients\n",
    "            \n",
    "        # End-of-epoch evaluation: Loss on complete dataset\n",
    "        predictions = X @ theta\n",
    "        loss = np.mean((predictions - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "            \n",
    "    return theta, losses\n",
    "\n",
    "# Apply SGD\n",
    "theta_sgd, losses_sgd = stochastic_gradient_descent(X_b, y, \n",
    "                                                   learning_rate=0.01,  # Much smaller!\n",
    "                                                   n_epochs=50)\n",
    "print(f\"SGD Parameters: {theta_sgd.flatten()[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**SGD Characteristics:**\n",
    "- **Extreme noise:** Each update uses only 1 sample out of 1000\n",
    "- **Need small learning rate:** Otherwise parameters bounce around wildly\n",
    "- **Multiple epochs:** Same data processed many times like repetition learning\n",
    "\n",
    "**When SGD excels:**\n",
    "- Real-time systems needing quick adaptation\n",
    "- Massive datasets that can't fit in memory\n",
    "- Nigerian health systems with streaming patient data\n",
    "\n",
    "**Reflection Question:** Compare SGD to how doctors update treatment plans - after each patient or after seeing many patients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis: All Methods Side-by-Side\n",
    "\n",
    "Now we compare accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn baseline: Established ML library for comparison\n",
    "sk_lr = LinearRegression()  # No hyperparameters needed for OLS equivalent\n",
    "sk_lr.fit(X, y.ravel())  # Fit on original X (no intercept added - sklearn handles internally)\n",
    "sk_predictions = sk_lr.predict(X)  # Generate predictions\n",
    "print(\"Scikit-learn Parameters (intercept + weights):\", sk_lr.intercept_, sk_lr.coef_)\n",
    "\n",
    "# Get predictions from our implementations\n",
    "ols_pred = X_b @ (theta_ols if theta_ols is not None else np.zeros((X_b.shape[1], 1)))\n",
    "bgd_pred = X_b @ theta_bgd\n",
    "mbgd_pred = X_b @ theta_mbgd  \n",
    "sgd_pred = X_b @ theta_sgd\n",
    "\n",
    "# Evaluation function\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics.\n",
    "    \n",
    "    MSE: Mean Squared Error - average squared prediction errors\n",
    "    RÂ²: Coefficient of determination - proportion of variance explained\n",
    "    \"\"\"  \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred) \n",
    "    return mse, r2\n",
    "\n",
    "# Compare all methods\n",
    "methods = ['Our OLS', 'Our BGD', 'Our Mini-batch', 'Our SGD', 'Scikit-learn']\n",
    "predictions = [ols_pred, bgd_pred, mbgd_pred, sgd_pred, sk_predictions.reshape(-1,1)]\n",
    "results = []\n",
    "\n",
    "print(\"\\nðŸŽ¯ Performance Comparison:\")\n",
    "print(\"Method\\t\\tMSE\\t\\tRÂ²\\t\\tMatch OLS?\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for method, pred in zip(methods, predictions):\n",
    "    mse, r2 = compute_metrics(y, pred)\n",
    "    # Check if close to OLS (our baseline)\n",
    "    ols_mse, ols_r2 = compute_metrics(y, ols_pred)\n",
    "    match_quality = \"Perfect\" if abs(mse - ols_mse) < 0.1 else \"Good\" if abs(mse - ols_mse) < 1.0 else \"Needs improvement\"\n",
    "    \n",
    "    print(f\"{method}\\t{mse:.4f}\\t{r2:.4f}\\t{match_quality}\")\n",
    "    results.append((mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** \n",
    "\n",
    "**Metrics Explained:**\n",
    "- **MSE (lower better):** Average squared prediction errors - like measuring lab accuracy\n",
    "- **RÂ² (higher better):** Percentage of target variation explained by our model\n",
    "- **Match to OLS:** OLS represents \"mathematical perfection\" - how close did iterative methods get?\n",
    "\n",
    "**Key Insights:** \n",
    "- GD methods should approach OLS quality with proper tuning\n",
    "- Trade-offs: Speed vs Accuracy vs Stability\n",
    "- Scikit-learn: Gold standard for verification\n",
    "\n",
    "**Healthcare Translation:** If OLS is a perfect diagnostic specialist, GD methods are lighting-fast general practitioners getting close to specialist accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Understanding the Learning Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss progression visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Subplot 1: BGD Loss (smooth but slow)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(losses_bgd[:100], 'b-')  # First 100 iterations\n",
    "plt.title('Batch GD: Loss Over Iterations\\n(Smooth, exact gradients)')\n",
    "plt.xlabel('Iteration Step')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.yscale('log')  # Log scale to show exponential improvement\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Mini-batch GD Loss (balanced approach)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(losses_mbgd, 'g-')\n",
    "plt.title('Mini-batch GD: Loss Per Epoch\\n(Fast convergence in batches)')\n",
    "plt.xlabel('Epoch (complete data passes)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: SGD Loss (noisy but fast)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(losses_sgd, 'r-')\n",
    "plt.title('SGD: Loss Per Epoch\\n(Noisy but computationally efficient)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Model predictive accuracy\n",
    "plt.subplot(2, 2, 4)\n",
    "# Compare OLS predictions to actual targets\n",
    "plt.scatter(y, ols_pred, alpha=0.4, color='purple', label='OLS Predictions')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', linewidth=2, label='Perfect Prediction Line')\n",
    "plt.xlabel('True Target Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Model Performance: OLS\\n(Our \"Gold Standard\")')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Loss Curves Interpretation:**\n",
    "- **Downward trend:** Shows learning progression\n",
    "- **Smoothness:** Batch GD = most stable, SGD = most variable\n",
    "- **Speed of descent:** How quickly error reduces\n",
    "\n",
    "**Prediction Plot:**\n",
    "- **Close to diagonal:** Good model performance\n",
    "- **Random scatter:** Poor model (no relationship found)\n",
    "\n",
    "**Visualization Tips:**\n",
    "- Log scale makes convergence clearer\n",
    "- Scatterplots reveal systematic bias\n",
    "- Compare curves to understand algorithm trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways and Nigerian Healthcare Applications\n",
    "\n",
    "**Algorithm Summary:**\n",
    "- **OLS:** Mathematically perfect but slow for big data\n",
    "- **Batch GD:** Stable, thorough, but needs full dataset in memory\n",
    "- **Mini-batch GD:** Practical compromise for most production systems\n",
    "- **SGD:** Fast, noisy, great for streaming data\n",
    "\n",
    "**Healthcare Translation - Mark:**\n",
    "Imagine building AI for Nigerian hospitals:\n",
    "- **Mini-batch GD:** Process patients in groups, balance speed/accuracy\n",
    "- **Early stopping:** Stop learning when performance stabilizes\n",
    "- **Real-time adaptation:** Use SGD for continuous health monitoring\n",
    "\n",
    "**Performance achieved:** Our basic implementations approach industry-level libraries!\n",
    "\n",
    "**Reflection Questions:**\n",
    "1. Which optimization method would you choose for a mobile health app monitoring Lagos patients?\n",
    "2. How might learning rate selection affect disease prediction reliability?\n",
    "3. Compare this to how Nigerian labs optimize diagnostic protocols through practice.\n",
    "\n",
    "**Next Steps:**\n",
    "- Add regularization to prevent overfitting\n",
    "- Extend to logistic regression for classification\n",
    "- Implement learning rate scheduling for faster convergence\n",
    "\n",
    "**ðŸ† Proud of your progress, my student! You've built the foundation of modern machine learning from scratch.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
