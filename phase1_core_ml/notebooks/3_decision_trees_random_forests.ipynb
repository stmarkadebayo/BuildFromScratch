{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees & Random Forests from Scratch\n",
    "\n",
    "**Welcome, St. Mark!** Now we explore tree-based methods. Think of this as building diagnostic decision trees - like the flowchart doctors use to diagnose illnesses.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "1. **Decision Tree Construction** - Building trees through recursive splitting\n",
    "2. **Splitting Criteria** - Gini impurity and information gain\n",
    "3. **Random Forests** - Ensemble learning through bagging\n",
    "4. **Feature Importance** - Understanding which features matter most\n",
    "\n",
    "By the end, you'll understand how ensemble methods achieve remarkable accuracy.\n",
    "\n",
    "## Core Definitions and Mathematical Foundations\n",
    "\n",
    "### Fundamental Definitions\n",
    "\n",
    "**Decision Tree:** A non-parametric supervised learning algorithm that partitions the feature space into regions and assigns each region a constant prediction value. Unlike parametric models (like logistic regression), decision trees adapt their complexity to the data structure without assuming any specific form of the relationship between features and target.\n",
    "\n",
    "**Key Components of a Decision Tree:**\n",
    "- **Root Node:** The topmost node representing the entire training dataset and the first decision point\n",
    "- **Internal Nodes:** Nodes that split data based on a feature threshold, creating decision branches\n",
    "- **Leaf Nodes:** Terminal nodes that provide final predictions (majority class for classification)\n",
    "- **Branches:** Decision paths connecting nodes based on whether feature values are â‰¤ or > a threshold\n",
    "\n",
    "**Random Forest:** An ensemble learning method that combines multiple decision trees through bootstrap aggregation (bagging) and feature randomness to reduce overfitting and improve generalization. The final prediction is typically the majority vote across all trees.\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "**Gini Impurity:** A quadratic measure of node impurity that quantifies how \"mixed\" a node is with respect to class labels.\n",
    "\n",
    "$$\\text{Gini}(D) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "\n",
    "Where:\n",
    "- $D$: Dataset at current node\n",
    "- $c$: Number of classes\n",
    "- $p_i$: Proportion of samples belonging to class $i$\n",
    "\n",
    "**Intuition:** Lower Gini = more pure node (better for splitting). A Gini of 0 means all samples in the node belong to the same class (perfect purity), while a Gini of 0.5 (for binary classification) represents maximum impurity (50-50 class distribution).\n",
    "\n",
    "**Entropy:** An alternative logarithmic measure of information content and impurity.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$\\text{Entropy}(D) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
    "\n",
    "**Information Gain:** The reduction in impurity achieved by splitting a node on a particular feature. The algorithm selects the split that maximizes information gain.\n",
    "\n",
    "$$\\text{Information Gain}(D, A) = \\text{Impurity}(D) - \\sum_{v \\in \\text{values}(A)} \\frac{|D_v|}{|D|} \\cdot \\text{Impurity}(D_v)$$\n",
    "\n",
    "Where:\n",
    "- $D$: Parent dataset\n",
    "- $A$: Feature to split on\n",
    "- $D_v$: Subset of $D$ where feature $A$ has value $v$\n",
    "\n",
    "**CART Algorithm:** Classification and Regression Trees - the specific algorithm we implement, characterized by:\n",
    "1. **Binary Splits:** Each internal node splits in exactly two child nodes\n",
    "2. **Greedy Algorithm:** Selects best split at each step using information gain\n",
    "3. **Impurity Minimization:** Uses Gini impurity (or optionally entropy) for split quality\n",
    "4. **Stopping Criteria:** Maximum depth, minimum samples per split, or pure nodes\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Decision trees are flowchart-like structures for classification:\n",
    "- **Root Node:** Starting decision point\n",
    "- **Internal Nodes:** Feature-based splits\n",
    "- **Leaf Nodes:** Final predictions\n",
    "- **Branches:** Decision paths based on feature thresholds\n",
    "\n",
    "**Key Question:** How do we automatically grow decision trees from data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Classification Dataset\n",
    "\n",
    "We'll use the same classification data as logistic regression for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Use same data as logistic regression for comparison\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                          n_features=4,\n",
    "                          n_classes=2,\n",
    "                          n_informative=3,\n",
    "                          n_redundant=1,\n",
    "                          n_clusters_per_class=1,\n",
    "                          random_state=42)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"Class distribution: {Counter(y_train)}\")\n",
    "print(f\"Feature ranges: min={X_train.min():.2f}, max={X_train.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** We've prepared our dataset.\n",
    "\n",
    "- **Same data:** Allows direct comparison with logistic regression\n",
    "- **Feature ranges:** Decision trees handle different scales naturally\n",
    "- **Class balance:** Important for tree construction\n",
    "\n",
    "**Healthcare Analogy:** Like preparing patient data for diagnostic flowchart creation.\n",
    "\n",
    "**Reflection Question:** Why might decision trees handle mixed data types better than logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Gini Impurity - Measuring Node Purity\n",
    "\n",
    "**Enhanced Definition:** Gini impurity quantifies the \"impurity\" or \"disorder\" of a node by measuring how evenly classes are distributed. It's a quadratic measure that reaches minimum (0) for pure nodes and maximum (0.5 for binary) for perfectly balanced splits.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$\\text{Gini}(D) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "\n",
    "Where:\n",
    "- $D$: Dataset at current node\n",
    "- $c$: Number of classes  \n",
    "- $p_i$: Proportion of class $i$ in dataset\n",
    "\n",
    "**Why Gini over Entropy?:**\n",
    "- **Computational efficiency:** Gini uses simple squaring vs entropy's logarithm\n",
    "- **Similar performance:** Both measure impurity effectively in practice\n",
    "- **C4.5 vs CART:** C4.5 uses entropy, CART (our algorithm) uses Gini\n",
    "\n",
    "Lower Gini = more pure node (better for splitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity for a node.\n",
    "    \n",
    "    Formula: Gini = 1 - Î£ p_iÂ² where p_i is proportion of class i\n",
    "    \n",
    "    Parameters:\n",
    "    y: Array of class labels for samples in this node\n",
    "    \n",
    "    Returns:\n",
    "    Gini impurity score (0 = pure, 0.5 = maximum impurity for binary)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count class frequencies\n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    \n",
    "    # Calculate Gini impurity: 1 - Î£pÂ²\n",
    "    gini = 1.0\n",
    "    for count in class_counts.values():\n",
    "        prob = count / total_samples\n",
    "        gini -= prob ** 2\n",
    "    \n",
    "    return gini\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate entropy (alternative to Gini).\n",
    "    \n",
    "    Formula: Entropy = -Î£ p_i * logâ‚‚(p_i)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    \n",
    "    # Calculate entropy with log base 2\n",
    "    entropy_val = 0.0\n",
    "    for count in class_counts.values():\n",
    "        if count > 0:\n",
    "            prob = count / total_samples\n",
    "            entropy_val -= prob * np.log2(prob)\n",
    "    \n",
    "    return entropy_val\n",
    "\n",
    "# Test impurity measures on different node compositions\n",
    "# Pure nodes have Gini = 0 (perfect certainty)\n",
    "# Mixed nodes have higher Gini (closer to 0.5)\n",
    "test_cases = [\n",
    "    ([0, 0, 0, 0], \"Pure class 0 - Perfect diagnosis\"),\n",
    "    ([1, 1, 1, 1], \"Pure class 1 - Confident prediction\"),\n",
    "    ([0, 1, 0, 1], \"50-50 mix - Maximum uncertainty\"),\n",
    "    ([0, 0, 1], \"Mostly healthy - Slight uncertainty\")\n",
    "]\n",
    "\n",
    "print(\"Impurity Measures Comparison - Healthcare Diagnosis Analogy:\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Node Composition':<15} {'Gini':<8} {'Entropy':<8} Diagnosis Certainty\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for labels, description in test_cases:\n",
    "    gini_val = gini_impurity(labels)\n",
    "    entropy_val = entropy(labels)\n",
    "    print(f\"{description:<15} {gini_val:<8.3f} {entropy_val:<8.3f} {str(labels)}\")\n",
    "\n",
    "# Visualize how Gini and entropy behave as class proportions change\n",
    "# This shows why Gini provides quadratic penalty for impurity\n",
    "proportions = np.linspace(0.01, 0.99, 100)\n",
    "gini_curve = [1 - p**2 - (1-p)**2 for p in proportions]\n",
    "entropy_curve = [-p*np.log2(p) - (1-p)*np.log2(1-p) for p in proportions]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(proportions, gini_curve, 'b-', linewidth=2, label='Gini Impurity (Quadratic)')\n",
    "plt.plot(proportions, entropy_curve, 'r-', linewidth=2, label='Entropy (Logarithmic)')\n",
    "plt.xlabel('Proportion of Class 1 in Node')\n",
    "plt.ylabel('Impurity Value')\n",
    "plt.title('Gini vs Entropy: Different Penalty Functions for Impurity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis - Detailed Gini Explanation:**\n",
    "\n",
    "**Impurity Measures Interpretation:**\n",
    "- **Gini = 0 (Pure nodes):** All samples belong to same class - diagnostic certainty\n",
    "- **Gini = 0.5 (50-50 split):** Maximum uncertainty - coin flip diagnosis  \n",
    "- **Gini > 0.5:** Imbalanced but multi-class mixing\n",
    "\n",
    "**Mathematical Properties:**\n",
    "- **Quadratic penalty:** Gini (p) for binary classes: 1 - pÂ² - (1-p)Â² = 2p(1-p)\n",
    "- **Maximum at p=0.5:** Gini reaches 0.5 for perfectly balanced classes\n",
    "- **Fast computation:** No logarithm required, suitable for large datasets\n",
    "\n",
    "**Healthcare Translation:** Like certainty in medical diagnosis:\n",
    "- Pure node (Gini=0): \"Patient definitely has Malaria\"\n",
    "- High Gini (0.5): \"Need more tests, completely uncertain\"\n",
    "- Mixed Gini: \"Might be Typhoid, but some Malaria indicators present\"\n",
    "\n",
    "**Computational Trade-offs:**\n",
    "- **Gini:** Faster (squares vs logs), insensitive to class probability extremes\n",
    "- **Entropy:** More \"aggressive\" penalty for changes, theoretically sounder in information theory\n",
    "\n",
    "**Reflection Question:** Why might the quadratic penalty in Gini be more practical for medical decision trees than logarithmic entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Finding Optimal Splits - Information Gain\n",
    "\n",
    "**Enhanced Definition:** We evaluate every possible feature split to find the one that maximizes information gain - the reduction in impurity achieved by partitioning the data.\n",
    "\n",
    "**Information Gain Formula:**\n",
    "$$\\text{Information Gain}(D, A) = \\text{Impurity}(D) - \\sum_{v \\in \\text{values}(A)} \\frac{|D_v|}{|D|} \\cdot \\text{Impurity}(D_v)$$\n",
    "\n",
    "Where:\n",
    "- $D$: Parent dataset (current node)\n",
    "- $A$: Feature to split on  \n",
    "- $D_v$: Data subset where feature $A$ has value $v$\n",
    "- The weighted average impurity of children is subtracted from parent impurity\n",
    "\n",
    "**Greedy Algorithm Concept:** Higher information gain = better split. The CART algorithm tries every possible feature and threshold combination to maximize this gain.\n",
    "\n",
    "**Why Midpoints Between Values?** The algorithm evaluates splits at the midpoint between consecutive sorted feature values. This creates binary splits that are:\n",
    "- **Deterministic:** Every sample goes left or right based on threshold\n",
    "- **Efficient:** Only considers points where splits can actually change\n",
    "- **Robust:** Avoids splits within identical feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, feature_indices=None):\n",
    "    \"\"\"\n",
    "    Find the best feature and threshold to split on using information gain maximization.\n",
    "    \n",
    "    This implements the CART algorithm's greedy split selection strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Feature matrix\n",
    "    y: Target labels \n",
    "    feature_indices: Subset of features to consider (for random forests)\n",
    "    \n",
    "    Returns:\n",
    "    best_feature, best_threshold, best_gain, best_splits\n",
    "    \"\"\"\n",
    "    if feature_indices is None:\n",
    "        feature_indices = range(X.shape[1])\n",
    "    \n",
    "    best_gain = 0  # Track the maximum information gain found\n",
    "    best_feature = None  # Feature index for best split\n",
    "    best_threshold = None  # Threshold value for best split\n",
    "    best_splits = None  # (left_labels, right_labels) for best split\n",
    "    \n",
    "    # Calculate parent node impurity once\n",
    "    parent_impurity = gini_impurity(y)\n",
    "    n_samples = len(y)\n",
    "    \n",
    "    # Greedy algorithm: Try all possible splits systematically\n",
    "    for feature_idx in feature_indices:\n",
    "        feature_values = X[:, feature_idx]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        \n",
    "        # CART strategy: Create thresholds at midpoints between consecutive values\n",
    "        # This ensures every sample goes deterministically left or right\n",
    "        sorted_values = np.sort(unique_values)\n",
    "        thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Binary split: samples â‰¤ threshold go left, > threshold go right\n",
    "            left_mask = feature_values <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            y_left = y[left_mask]\n",
    "            y_right = y[right_mask]\n",
    "            \n",
    "            # Skip trivial splits (empty child nodes)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted average impurity of child nodes\n",
    "            left_impurity = gini_impurity(y_left)\n",
    "            right_impurity = gini_impurity(y_right)\n",
    "            \n",
    "            left_weight = len(y_left) / n_samples\n",
    "            right_weight = len(y_right) / n_samples\n",
    "            \n",
    "            # Weighted average = (weight_left Ã— impurity_left) + (weight_right Ã— impurity_right)  \n",
    "            weighted_child_impurity = (left_weight * left_impurity + \n",
    "                                      right_weight * right_impurity)\n",
    "            \n",
    "            # Information gain = parent_impurity - weighted_child_impurity\n",
    "            # Higher gain = better split (more impurity reduction)\n",
    "            information_gain = parent_impurity - weighted_child_impurity\n",
    "            \n",
    "            # Update best split if this split achieves highest gain\n",
    "            if information_gain > best_gain:\n",
    "                best_gain = information_gain\n",
    "                best_feature = feature_idx\n",
    "                best_threshold = threshold\n",
    "                best_splits = (y_left, y_right)\n",
    "    \n",
    "    return best_feature, best_threshold, best_gain, best_splits\n",
    "\n",
    "# Demonstrate how split finding works with information gain\n",
    "feature_idx, threshold, gain, splits = find_best_split(X_train, y_train)\n",
    "\n",
    "print(f\"ðŸ” Optimal Split Found Using Information Gain:\")\n",
    "print(f\"Selected Feature: {feature_idx}\")\n",
    "print(f\"Split Threshold: {threshold:.3f} (values â‰¤ {threshold:.3f} go left)\")\n",
    "print(f\"Information Gain Achieved: {gain:.4f}\")\n",
    "print(f\"Left Child: {len(splits[0])} samples, Gini: {gini_impurity(splits[0]):.3f}, Classes: {Counter(splits[0])}\")\n",
    "print(f\"Right Child: {len(splits[1])} samples, Gini: {gini_impurity(splits[1]):.3f}, Classes: {Counter(splits[1])}\")\n",
    "print(f\"Total Child Gini (weighted): {(len(splits[0])/len(y_train)*gini_impurity(splits[0]) + len(splits[1])/len(y_train)*gini_impurity(splits[1])):.3f}\")\n",
    "\n",
    "# Visualize the information gain maximization in action\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot all data points with class colors\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=0.6, cmap='RdYlBu', s=50)\n",
    "\n",
    "# Highlight the chosen split feature and threshold\n",
    "# This split maximizes information gain (minimizes weighted child impurity)\n",
    "if feature_idx == 0:\n",
    "    plt.axvline(x=threshold, color='red', linewidth=3, label=f'Information Gain Split: Feature {feature_idx} â‰¤ {threshold:.3f}')\n",
    "    # Add annotation for information gain\n",
    "    plt.annotate(f'Information Gain: {gain:.3f}\\n(split maximizes purity improvement)', \n",
    "                xy=(threshold, 0.5), xycoords='data',\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8),\n",
    "                fontsize=9, ha='left')\n",
    "else:\n",
    "    plt.axhline(y=threshold, color='red', linewidth=3, label=f'Information Gain Split: Feature {feature_idx} â‰¤ {threshold:.3f}')\n",
    "    plt.annotate(f'Information Gain: {gain:.3f}\\n(split maximizes purity improvement)', \n",
    "                xy=(threshold, 0.5), xycoords='data',\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8),\n",
    "                fontsize=9, ha='left')\n",
    "\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1') \n",
    "plt.title('Decision Tree: Information Gain Maximization\\\\n(Red line shows split that maximizes impurity reduction)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis - Information Gain Deep Dive:**\n",
    "\n",
    "**Algorithm Mechanics:**\n",
    "- **Exhaustive Search:** Systematically evaluates all features and possible thresholds\n",
    "- **Midpoint Strategy:** Splits at midpoints between consecutive feature values\n",
    "- **Gain Calculation:** Measures how much weighted impurity decreases after split\n",
    "- **Greedy Selection:** Chooses split that maximizes information gain\n",
    "\n",
    "**Mathematical Rationale for Midpoint Thresholds:**\n",
    "- **Avoids no-change splits:** Splitting within a cluster of identical values has no effect\n",
    "- **Binary decisions:** Each sample definitively goes left or right\n",
    "- **Efficiency:** Reduces number of candidate splits to O(n log n) vs O(nÂ²)\n",
    "- **Determinism:** Eliminates ambiguity in split boundaries\n",
    "\n",
    "**Why Midpoints Matter in Healthcare:**\n",
    "- **Quantitative features:** Lab values, vital signs have natural split points\n",
    "- **Binary decisions:** \"Is blood pressure â‰¤ 140?\" vs \"Is it â‰¤ 138 or â‰¤ 142?\"\n",
    "- **Clinical thresholds:** Often based on standard cutoffs in medical guidelines\n",
    "\n",
    "**Computational Complexity:**\n",
    "- **Per-node cost:** O(F Ã— V Ã— N) where F=features, V=unique values, N=samples\n",
    "- **CART optimization:** Midpoint strategy reduces V to O(N) in practice\n",
    "- **Trade-off:** Computationally expensive but finds optimal splits\n",
    "\n",
    "**Healthcare Interpretation:** Like choosing the most diagnostic question:\n",
    "- \"Does patient have fever > 38Â°C?\" vs \"Does patient have fever > 37Â°C?\"\n",
    "- Information gain quantifies how well the question separates diseases\n",
    "\n",
    "**Reflection Question:** Why would midpoints between feature values create more deterministic decision boundaries than arbitrary thresholds?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
