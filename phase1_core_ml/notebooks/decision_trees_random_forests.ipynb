{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees & Random Forests from Scratch\n",
    "\n",
    "**Welcome, St. Mark!** Now we explore tree-based methods. Think of this as building diagnostic decision trees - like the flowchart doctors use to diagnose illnesses.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "1. **Decision Tree Construction** - Building trees through recursive splitting\n",
    "2. **Splitting Criteria** - Gini impurity and information gain\n",
    "3. **Random Forests** - Ensemble learning through bagging\n",
    "4. **Feature Importance** - Understanding which features matter most\n",
    "\n",
    "By the end, you'll understand how ensemble methods achieve remarkable accuracy.\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Decision trees are flowchart-like structures for classification:\n",
    "- **Root Node:** Starting decision point\n",
    "- **Internal Nodes:** Feature-based splits\n",
    "- **Leaf Nodes:** Final predictions\n",
    "- **Branches:** Decision paths based on feature thresholds\n",
    "\n",
    "**Key Question:** How do we automatically grow decision trees from data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Classification Dataset\n",
    "\n",
    "We'll use the same classification data as logistic regression for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Use same data as logistic regression for comparison\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                          n_features=4,\n",
    "                          n_classes=2,\n",
    "                          n_informative=3,\n",
    "                          n_redundant=1,\n",
    "                          n_clusters_per_class=1,\n",
    "                          random_state=42)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"Class distribution: {Counter(y_train)}\")\n",
    "print(f\"Feature ranges: min={X_train.min():.2f}, max={X_train.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** We've prepared our dataset.\n",
    "\n",
    "- **Same data:** Allows direct comparison with logistic regression\n",
    "- **Feature ranges:** Decision trees handle different scales naturally\n",
    "- **Class balance:** Important for tree construction\n",
    "\n",
    "**Healthcare Analogy:** Like preparing patient data for diagnostic flowchart creation.\n",
    "\n",
    "**Reflection Question:** Why might decision trees handle mixed data types better than logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Gini Impurity - Measuring Node Purity\n",
    "\n",
    "Gini impurity measures how \"mixed\" a node is:\n",
    "\n",
    "**$$\\text{Gini} = 1 - \\sum p_i^2$$**\n",
    "\n",
    "Lower Gini = more pure node (better for splitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity for a node.\n",
    "    \n",
    "    Gini = 1 - Î£ p_iÂ² where p_i is proportion of class i\n",
    "    \n",
    "    Parameters:\n",
    "    y: Array of class labels for samples in this node\n",
    "    \n",
    "    Returns:\n",
    "    Gini impurity score (0 = pure, 0.5 = maximum impurity for binary)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count class frequencies\n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    \n",
    "    # Calculate Gini impurity\n",
    "    gini = 1.0\n",
    "    for count in class_counts.values():\n",
    "        prob = count / total_samples\n",
    "        gini -= prob ** 2\n",
    "    \n",
    "    return gini\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate entropy (alternative to Gini).\n",
    "    \n",
    "    Entropy = -Î£ p_i * logâ‚‚(p_i)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    \n",
    "    entropy_val = 0.0\n",
    "    for count in class_counts.values():\n",
    "        if count > 0:\n",
    "            prob = count / total_samples\n",
    "            entropy_val -= prob * np.log2(prob)\n",
    "    \n",
    "    return entropy_val\n",
    "\n",
    "# Test impurity measures\n",
    "test_cases = [\n",
    "    ([0, 0, 0, 0], \"Pure class 0\"),\n",
    "    ([1, 1, 1, 1], \"Pure class 1\"),\n",
    "    ([0, 1, 0, 1], \"Perfect mix\"),\n",
    "    ([0, 0, 1], \"Mostly class 0\")\n",
    "]\n",
    "\n",
    "print(\"Impurity Measures Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Case':<15} {'Gini':<8} {'Entropy':<8} Description\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for labels, desc in test_cases:\n",
    "    gini_val = gini_impurity(labels)\n",
    "    entropy_val = entropy(labels)\n",
    "    print(f\"{desc:<15} {gini_val:<8.3f} {entropy_val:<8.3f} {str(labels)}\")\n",
    "\n",
    "# Visualize impurity vs class proportion\n",
    "proportions = np.linspace(0.01, 0.99, 100)\n",
    "gini_curve = [1 - p**2 - (1-p)**2 for p in proportions]\n",
    "entropy_curve = [-p*np.log2(p) - (1-p)*np.log2(1-p) for p in proportions]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(proportions, gini_curve, 'b-', linewidth=2, label='Gini Impurity')\n",
    "plt.plot(proportions, entropy_curve, 'r-', linewidth=2, label='Entropy')\n",
    "plt.xlabel('Proportion of Class 1')\n",
    "plt.ylabel('Impurity Value')\n",
    "plt.title('Gini vs Entropy: Impurity Measures')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Impurity Measures:**\n",
    "- **Gini:** Quadratic penalty for impurity\n",
    "- **Entropy:** Logarithmic penalty (more sensitive to changes)\n",
    "- **Range:** Both 0 (pure) to max (balanced mix)\n",
    "\n",
    "**Healthcare Analogy:** Like measuring diagnostic certainty - pure nodes are clear diagnoses.\n",
    "\n",
    "**Reflection Question:** Why might Gini be preferred over entropy in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Finding Optimal Splits\n",
    "\n",
    "We evaluate every possible split to find the best one:\n",
    "\n",
    "**$$\\text{Information Gain} = \\text{Gini}(\\text{parent}) - \\text{weighted\\_avg}(\\text{Gini}(\\text{children}))$$**\n",
    "\n",
    "Higher gain = better split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, feature_indices=None):\n",
    "    \"\"\"\n",
    "    Find the best feature and threshold to split on.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Feature matrix\n",
    "    y: Target labels\n",
    "    feature_indices: Subset of features to consider (for random forests)\n",
    "    \n",
    "    Returns:\n",
    "    best_feature, best_threshold, best_gain, best_splits\n",
    "    \"\"\"\n",
    "    if feature_indices is None:\n",
    "        feature_indices = range(X.shape[1])\n",
    "    \n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_splits = None\n",
    "    \n",
    "    # Calculate parent impurity\n",
    "    parent_impurity = gini_impurity(y)\n",
    "    n_samples = len(y)\n",
    "    \n",
    "    # Try each feature\n",
    "    for feature_idx in feature_indices:\n",
    "        feature_values = X[:, feature_idx]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        \n",
    "        # Try different thresholds (midpoints between sorted values)\n",
    "        sorted_values = np.sort(unique_values)\n",
    "        thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Split data\n",
    "            left_mask = feature_values <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            y_left = y[left_mask]\n",
    "            y_right = y[right_mask]\n",
    "            \n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted impurity of children\n",
    "            left_impurity = gini_impurity(y_left)\n",
    "            right_impurity = gini_impurity(y_right)\n",
    "            \n",
    "            left_weight = len(y_left) / n_samples\n",
    "            right_weight = len(y_right) / n_samples\n",
    "            \n",
    "            weighted_impurity = (left_weight * left_impurity + \n",
    "                               right_weight * right_impurity)\n",
    "            \n",
    "            # Information gain\n",
    "            gain = parent_impurity - weighted_impurity\n",
    "            \n",
    "            # Track best split\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature_idx\n",
    "                best_threshold = threshold\n",
    "                best_splits = (y_left, y_right)\n",
    "    \n",
    "    return best_feature, best_threshold, best_gain, best_splits\n",
    "\n",
    "# Demonstrate split finding\n",
    "feature_idx, threshold, gain, splits = find_best_split(X_train, y_train)\n",
    "\n",
    "print(f\"Best split found:\")\n",
    "print(f\"Feature {feature_idx}, Threshold: {threshold:.3f}\")\n",
    "print(f\"Information Gain: {gain:.4f}\")\n",
    "print(f\"Left split: {len(splits[0])} samples, classes: {Counter(splits[0])}\")\n",
    "print(f\"Right split: {len(splits[1])} samples, classes: {Counter(splits[1])}\")\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot all data points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=0.6, cmap='RdYlBu', s=50)\n",
    "\n",
    "# Highlight the best split feature\n",
    "if feature_idx == 0:\n",
    "    plt.axvline(x=threshold, color='red', linewidth=3, label=f'Best split: Feature {feature_idx} â‰¤ {threshold:.3f}')\n",
    "else:\n",
    "    plt.axhline(y=threshold, color='red', linewidth=3, label=f'Best split: Feature {feature_idx} â‰¤ {threshold:.3f}')\n",
    "\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.title('Decision Tree: Best First Split')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Split Finding Process:**\n",
    "- **Exhaustive search:** Try all features and thresholds\n",
    "- **Information gain:** Measure impurity reduction\n",
    "- **Best split:** Maximizes class separation\n",
    "\n",
    "**Healthcare Analogy:** Like choosing the most diagnostic symptom to ask about first.\n",
    "\n",
    "**Reflection Question:** Why do we use midpoints between values as thresholds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Building the Decision Tree\n",
    "\n",
    "Now we recursively build the tree using the CART algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"\n",
    "    Node in our decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Feature index to split on\n",
    "        self.threshold = threshold  # Split threshold\n",
    "        self.left = left           # Left subtree (â‰¤ threshold)\n",
    "        self.right = right         # Right subtree (> threshold)\n",
    "        self.value = value         # Leaf node prediction (majority class)\n",
    "\n",
    "def build_decision_tree(X, y, max_depth=5, min_samples_split=2, feature_indices=None):\n",
    "    \"\"\"\n",
    "    Recursively build a decision tree using CART algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    X, y: Training data\n",
    "    max_depth: Maximum tree depth\n",
    "    min_samples_split: Minimum samples needed to split\n",
    "    feature_indices: Feature subset (for random forests)\n",
    "    \"\"\"\n",
    "    # Base cases\n",
    "    if (len(y) < min_samples_split or \n",
    "        max_depth == 0 or \n",
    "        len(np.unique(y)) == 1):  # Pure node\n",
    "        \n",
    "        # Create leaf node with majority class\n",
    "        majority_class = Counter(y).most_common(1)[0][0]\n",
    "        return DecisionTreeNode(value=majority_class)\n",
    "    \n",
    "    # Find best split\n",
    "    feature, threshold, gain, _ = find_best_split(X, y, feature_indices)\n",
    "    \n",
    "    # If no good split found, create leaf\n",
    "    if feature is None or gain == 0:\n",
    "        majority_class = Counter(y).most_common(1)[0][0]\n",
    "        return DecisionTreeNode(value=majority_class)\n",
    "    \n",
    "    # Split data\n",
    "    left_mask = X[:, feature] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "    \n",
    "    X_left, y_left = X[left_mask], y[left_mask]\n",
    "    X_right, y_right = X[right_mask], y[right_mask]\n",
    "    \n",
    "    # Recursively build subtrees\n",
    "    left_subtree = build_decision_tree(X_left, y_left, max_depth-1, \n",
    "                                       min_samples_split, feature_indices)\n",
    "    right_subtree = build_decision_tree(X_right, y_right, max_depth-1,\n",
    "                                        min_samples_split, feature_indices)\n",
    "    \n",
    "    # Return internal node\n",
    "    return DecisionTreeNode(feature=feature, threshold=threshold,\n",
    "                           left=left_subtree, right=right_subtree)\n",
    "\n",
    "def predict_tree(node, x):\n",
    "    \"\"\"\n",
    "    Make prediction for a single sample.\n",
    "    \"\"\"\n",
    "    # Traverse tree until leaf\n",
    "    while node.value is None:\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    return node.value\n",
    "\n",
    "def predict_tree_batch(tree, X):\n",
    "    \"\"\"\n",
    "    Make predictions for multiple samples.\n",
    "    \"\"\"\n",
    "    return np.array([predict_tree(tree, x) for x in X])\n",
    "\n",
    "# Build our decision tree\n",
    "print(\"Building decision tree...\")\n",
    "start_time = time.time()\n",
    "tree = build_decision_tree(X_train, y_train, max_depth=5)\n",
    "build_time = time.time() - start_time\n",
    "print(f\"Tree built in {build_time:.3f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = predict_tree_batch(tree, X_train)\n",
    "test_predictions = predict_tree_batch(tree, X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nDecision Tree Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Overfitting gap: {train_accuracy - test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Tree Building:**\n",
    "- **Recursive construction:** Each node splits optimally\n",
    "- **Stopping criteria:** Prevent overfitting\n",
    "- **Traversal:** Simple path to leaf prediction\n",
    "\n",
    "**Healthcare Analogy:** Like building a diagnostic flowchart through recursive refinement.\n",
    "\n",
    "**Reflection Question:** How does tree depth affect overfitting vs underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Random Forest - Ensemble Learning\n",
    "\n",
    "Random forests combine multiple trees through bagging:\n",
    "- **Bootstrap sampling:** Random subsets of data\n",
    "- **Feature randomness:** Random feature subsets per split\n",
    "- **Majority voting:** Combine tree predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest implementation with bagging and feature randomness.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2, max_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features  # None = sqrt(n_features)\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the random forest.\"\"\"\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Default max_features to sqrt(n_features)\n",
    "        if self.max_features is None:\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        \n",
    "        print(f\"Training {self.n_trees} trees with max_features={self.max_features}...\")\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            # Bootstrap sampling\n",
    "            bootstrap_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_bootstrap = X[bootstrap_indices]\n",
    "            y_bootstrap = y[bootstrap_indices]\n",
    "            \n",
    "            # Random feature subset for this tree\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "            \n",
    "            # Build tree\n",
    "            tree = build_decision_tree(X_bootstrap, y_bootstrap, \n",
    "                                      self.max_depth, self.min_samples_split,\n",
    "                                      feature_indices)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Trained {i + 1}/{self.n_trees} trees\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using majority voting.\"\"\"\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([predict_tree_batch(tree, X) for tree in self.trees])\n",
    "        \n",
    "        # Majority voting\n",
    "        final_predictions = []\n",
    "        for sample_idx in range(X.shape[0]):\n",
    "            sample_votes = tree_predictions[:, sample_idx]\n",
    "            majority_vote = Counter(sample_votes).most_common(1)[0][0]\n",
    "            final_predictions.append(majority_vote)\n",
    "        \n",
    "        return np.array(final_predictions)\n",
    "\n",
    "# Train our random forest\n",
    "rf = RandomForest(n_trees=10, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_train_predictions = rf.predict(X_train)\n",
    "rf_test_predictions = rf.predict(X_test)\n",
    "\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_predictions)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_test_predictions)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"Training Accuracy: {rf_train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {rf_test_accuracy:.4f}\")\n",
    "print(f\"Overfitting gap: {rf_train_accuracy - rf_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Random Forest Process:**\n",
    "- **Bootstrap sampling:** Each tree sees different data\n",
    "- **Feature randomness:** Each split considers subset of features\n",
    "- **Ensemble voting:** Reduces overfitting through averaging\n",
    "\n",
    "**Healthcare Analogy:** Like consulting multiple doctors - each sees different patients but their consensus is more reliable.\n",
    "\n",
    "**Reflection Question:** Why does randomness in random forests prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Decision Tree vs Random Forest vs Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn baselines\n",
    "sk_dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "sk_dt.fit(X_train, y_train)\n",
    "sk_dt_train_acc = accuracy_score(y_train, sk_dt.predict(X_train))\n",
    "sk_dt_test_acc = accuracy_score(y_test, sk_dt.predict(X_test))\n",
    "\n",
    "sk_rf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "sk_rf.fit(X_train, y_train)\n",
    "sk_rf_train_acc = accuracy_score(y_train, sk_rf.predict(X_train))\n",
    "sk_rf_test_acc = accuracy_score(y_test, sk_rf.predict(X_test))\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nðŸŽ¯ Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<20} {'Train Acc':<10} {'Test Acc':<10} {'Overfit Gap':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "methods_data = [\n",
    "    ('Our Decision Tree', train_accuracy, test_accuracy),\n",
    "    ('Our Random Forest', rf_train_accuracy, rf_test_accuracy),\n",
    "    ('Sklearn DecisionTree', sk_dt_train_acc, sk_dt_test_acc),\n",
    "    ('Sklearn RandomForest', sk_rf_train_acc, sk_rf_test_acc)\n",
    "]\n",
    "\n",
    "for method, train_acc, test_acc in methods_data:\n",
    "    gap = train_acc - test_acc\n",
    "    print(f\"{method:<20} {train_acc:<10.4f} {test_acc:<10.4f} {gap:<12.4f}\")\n",
    "\n",
    "# Classification reports\n",
    "print(\"\\nOur Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_test_predictions))\n",
    "\n",
    "print(\"\\nScikit-learn Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, sk_rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Performance Comparison:**\n",
    "- **Decision Tree:** Single tree, prone to overfitting\n",
    "- **Random Forest:** Ensemble reduces overfitting, better generalization\n",
    "- **Our vs Sklearn:** Our implementations approach library performance\n",
    "\n",
    "**Healthcare Translation:** Random forests like getting second opinions from multiple specialists.\n",
    "\n",
    "**Reflection Question:** Why do random forests typically outperform single decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways and Nigerian Healthcare Applications\n",
    "\n",
    "**Algorithm Summary:**\n",
    "- **Decision Trees:** Recursive binary splitting using impurity measures\n",
    "- **Gini Impurity:** Quadratic measure of node purity\n",
    "- **Random Forests:** Bootstrap aggregation with feature randomness\n",
    "- **Ensemble Learning:** Combining multiple models for better performance\n",
    "\n",
    "**Healthcare Translation - Mark:**\n",
    "\n",
    "Imagine building AI for Nigerian hospitals:\n",
    "- **Decision Trees:** Clear diagnostic rules doctors can understand\n",
    "- **Random Forests:** Robust predictions across different patient populations\n",
    "- **Feature Importance:** Which symptoms are most diagnostic\n",
    "- **Interpretability:** Unlike neural networks, trees explain their reasoning\n",
    "\n",
    "**Performance achieved:** Our implementations match industry standards!\n",
    "\n",
    "**Reflection Questions:**\n",
    "1. How would you use decision trees for Nigerian disease classification?\n",
    "2. Why might random forests be preferable to single trees in medical diagnosis?\n",
    "3. How does tree interpretability help with healthcare trust and adoption?\n",
    "\n",
    "**Next Steps:**\n",
    "- Add support for categorical features\n",
    "- Implement pruning techniques\n",
    "- Explore gradient boosting methods\n",
    "\n",
    "**ðŸ† Outstanding work, my student! You've mastered tree-based learning from the ground up.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
