{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Variants: Momentum, Adam, and Beyond\n",
    "\n",
    "**Welcome back, St. Mark!** Today we explore the optimization algorithms that make modern machine learning possible. Think of gradient descent variants as different \"learning strategies\" - from the steady plodding of basic GD to the adaptive intelligence of Adam.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "1. **Vanilla Gradient Descent** - The foundation of all optimization\n",
    "2. **Momentum** - Accelerating convergence with physics-inspired motion\n",
    "3. **RMSProp** - Adaptive learning rates for each parameter\n",
    "4. **Adam** - The \"best of both worlds\" optimizer\n",
    "5. **Comparison** - When to use each approach\n",
    "\n",
    "By the end, you'll understand why optimization algorithms are the \"engines\" of machine learning.\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "**Optimization Challenge:**\n",
    "- **High-dimensional landscapes:** Loss functions with thousands of parameters\n",
    "- **Local minima and saddle points:** Complex optimization surfaces\n",
    "- **Computational constraints:** Need efficient convergence\n",
    "- **Generalization:** Different optimizers affect model performance\n",
    "\n",
    "**Evolution of Optimizers:**\n",
    "- **Vanilla GD:** Simple but slow, gets stuck in ravines\n",
    "- **Momentum:** Adds velocity, escapes local minima\n",
    "- **Adaptive methods:** Adjust learning rates per parameter\n",
    "- **Modern approaches:** Combine momentum with adaptivity\n",
    "\n",
    "**Key Question:** How can we efficiently navigate the complex loss landscapes of medical AI models?\n",
    "\n",
    "## Data Preparation: Optimization Benchmark Dataset\n",
    "\n",
    "We'll use a synthetic dataset to compare optimizer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create binary classification dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
    "                          n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "# Convert to neural network format\n",
    "def to_nn_format(X, y):\n",
    "    \"\"\"Convert to format suitable for neural network training.\"\"\"\n",
    "    # Add bias term as additional feature\n",
    "    X_nn = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias column\n",
    "    y_nn = y.reshape(-1, 1)  # Make column vector\n",
    "    return X_nn, y_nn\n",
    "\n",
    "X_train_nn, y_train_nn = to_nn_format(X_train_scaled, y_train)\n",
    "X_test_nn, y_test_nn = to_nn_format(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Neural network format: X={X_train_nn.shape}, y={y_train_nn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** We've prepared our optimization benchmark dataset.\n",
    "\n",
    "- **Binary classification:** Clean evaluation of optimizer performance\n",
    "- **Feature scaling:** Important for stable optimization\n",
    "- **Neural network format:** Ready for logistic regression training\n",
    "\n",
    "**Reflection Question:** Why is feature scaling crucial for optimization algorithms?\n",
    "\n",
    "## Method 1: Vanilla Gradient Descent\n",
    "\n",
    "**Core idea:** Take small steps downhill in the direction of steepest descent.\n",
    "\n",
    "**Mathematical foundation:** $\\theta = \\theta - \\eta \\nabla J(\\theta)$\n",
    "\n",
    "**Limitations:** Slow convergence, gets stuck in ravines, sensitive to learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def logistic_loss(y_true, y_pred):\n",
    "    \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def logistic_gradients(X, y, w):\n",
    "    \"\"\"Compute gradients for logistic regression.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    y_pred = sigmoid(X @ w)\n",
    "    errors = y_pred - y\n",
    "    gradients = (1/n_samples) * X.T @ errors\n",
    "    return gradients\n",
    "\n",
    "class VanillaGD:\n",
    "    \"\"\"Vanilla Gradient Descent optimizer.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, w, gradients):\n",
    "        \"\"\"Update parameters using vanilla GD.\"\"\"\n",
    "        return w - self.learning_rate * gradients\n",
    "\n",
    "# Test vanilla GD\n",
    "print(\"Testing Vanilla Gradient Descent:\")\n",
    "w_init = np.random.randn(X_train_nn.shape[1], 1) * 0.01\n",
    "optimizer_vanilla = VanillaGD(learning_rate=0.1)\n",
    "\n",
    "losses_vanilla = []\n",
    "accuracies_vanilla = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred_train = sigmoid(X_train_nn @ w_init)\n",
    "\n",
    "    # Compute loss and accuracy\n",
    "    loss = logistic_loss(y_train_nn, y_pred_train)\n",
    "    predictions = (y_pred_train > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_train_nn, predictions)\n",
    "\n",
    "    losses_vanilla.append(loss)\n",
    "    accuracies_vanilla.append(accuracy)\n",
    "\n",
    "    # Backward pass\n",
    "    gradients = logistic_gradients(X_train_nn, y_train_nn, w_init)\n",
    "\n",
    "    # Update parameters\n",
    "    w_init = optimizer_vanilla.update(w_init, gradients)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Final: Loss = {losses_vanilla[-1]:.4f}, Accuracy = {accuracies_vanilla[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** Vanilla GD implementation complete.\n",
    "\n",
    "- **Steady improvement:** Loss decreases consistently over epochs\n",
    "- **Convergence:** Approaches optimal solution with sufficient iterations\n",
    "- **Limitations:** May be slow for complex problems\n",
    "\n",
    "**Reflection Question:** Why might vanilla GD struggle with ravine-like loss surfaces?\n",
    "\n",
    "## Method 2: Momentum - Physics-Inspired Optimization\n",
    "\n",
    "**Core idea:** Add velocity to accumulate gradient direction over time.\n",
    "\n",
    "**Mathematical foundation:**\n",
    "$v = \\gamma v + \\eta \\nabla J(\\theta)$\n",
    "$\\theta = \\theta - v$\n",
    "\n",
    "**Benefits:** Accelerates in consistent directions, dampens oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumGD:\n",
    "    \"\"\"Momentum Gradient Descent optimizer.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "\n",
    "    def update(self, w, gradients):\n",
    "        \"\"\"Update parameters using momentum.\"\"\"\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(w)\n",
    "\n",
    "        # Update velocity\n",
    "        self.velocity = self.momentum * self.velocity + self.learning_rate * gradients\n",
    "\n",
    "        # Update parameters\n",
    "        return w - self.velocity\n",
    "\n",
    "# Test momentum GD\n",
    "print(\"\\nTesting Momentum Gradient Descent:\")\n",
    "w_momentum = np.random.randn(X_train_nn.shape[1], 1) * 0.01\n",
    "optimizer_momentum = MomentumGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "losses_momentum = []\n",
    "accuracies_momentum = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred_train = sigmoid(X_train_nn @ w_momentum)\n",
    "\n",
    "    # Compute loss and accuracy\n",
    "    loss = logistic_loss(y_train_nn, y_pred_train)\n",
    "    predictions = (y_pred_train > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_train_nn, predictions)\n",
    "\n",
    "    losses_momentum.append(loss)\n",
    "    accuracies_momentum.append(accuracy)\n",
    "\n",
    "    # Backward pass\n",
    "    gradients = logistic_gradients(X_train_nn, y_train_nn, w_momentum)\n",
    "\n",
    "    # Update parameters\n",
    "    w_momentum = optimizer_momentum.update(w_momentum, gradients)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Final: Loss = {losses_momentum[-1]:.4f}, Accuracy = {accuracies_momentum[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** Momentum GD implementation complete.\n",
    "\n",
    "- **Faster convergence:** Often reaches better solutions in fewer iterations\n",
    "- **Smoother optimization:** Velocity accumulation reduces oscillations\n",
    "- **Medical analogy:** Like momentum in learning - builds expertise over time\n",
    "\n",
    "**Reflection Question:** How does momentum help escape local minima?\n",
    "\n",
    "## Method 3: Adam - Adaptive Moment Estimation\n",
    "\n",
    "**Core idea:** Combine momentum with adaptive learning rates.\n",
    "\n",
    "**Mathematical foundation:**\n",
    "$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla J(\\theta)$\n",
    "$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla J(\\theta))^2$\n",
    "$\\theta = \\theta - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$\n",
    "\n",
    "**Benefits:** Best of both worlds - momentum and adaptivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"Adam optimizer.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment (momentum)\n",
    "        self.v = None  # Second moment (RMSProp)\n",
    "        self.t = 0     # Time step\n",
    "\n",
    "    def update(self, w, gradients):\n",
    "        \"\"\"Update parameters using Adam.\"\"\"\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(w)\n",
    "            self.v = np.zeros_like(w)\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "\n",
    "        # Update biased second moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)\n",
    "\n",
    "        # Compute bias-corrected moments\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update parameters\n",
    "        return w - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "# Test Adam optimizer\n",
    "print(\"\\nTesting Adam Optimizer:\")\n",
    "w_adam = np.random.randn(X_train_nn.shape[1], 1) * 0.01\n",
    "optimizer_adam = Adam(learning_rate=0.01, beta1=0.9, beta2=0.999)\n",
    "\n",
    "losses_adam = []\n",
    "accuracies_adam = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred_train = sigmoid(X_train_nn @ w_adam)\n",
    "\n",
    "    # Compute loss and accuracy\n",
    "    loss = logistic_loss(y_train_nn, y_pred_train)\n",
    "    predictions = (y_pred_train > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_train_nn, predictions)\n",
    "\n",
    "    losses_adam.append(loss)\n",
    "    accuracies_adam.append(accuracy)\n",
    "\n",
    "    # Backward pass\n",
    "    gradients = logistic_gradients(X_train_nn, y_train_nn, w_adam)\n",
    "\n",
    "    # Update parameters\n",
    "    w_adam = optimizer_adam.update(w_adam, gradients)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Final: Loss = {losses_adam[-1]:.4f}, Accuracy = {accuracies_adam[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** Adam optimizer implementation complete.\n",
    "\n",
    "- **Adaptive learning:** Different learning rates for different parameters\n",
    "- **Bias correction:** Accounts for initialization bias in moment estimates\n",
    "- **Robust convergence:** Works well across different problem types\n",
    "\n",
    "**Reflection Question:** Why does Adam perform well on most deep learning problems?\n",
    "\n",
    "## Comparative Analysis: Optimizer Performance Comparison\n",
    "\n",
    "Let's visualize and compare all optimizers side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss comparison\n",
    "epochs = range(1, 101)\n",
    "ax1.plot(epochs, losses_vanilla, 'b-', label='Vanilla GD', linewidth=2)\n",
    "ax1.plot(epochs, losses_momentum, 'g-', label='Momentum GD', linewidth=2)\n",
    "ax1.plot(epochs, losses_adam, 'r-', label='Adam', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Optimizer Loss Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Accuracy comparison\n",
    "ax2.plot(epochs, accuracies_vanilla, 'b-', label='Vanilla GD', linewidth=2)\n",
    "ax2.plot(epochs, accuracies_momentum, 'g-', label='Momentum GD', linewidth=2)\n",
    "ax2.plot(epochs, accuracies_adam, 'r-', label='Adam', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Training Accuracy')\n",
    "ax2.set_title('Optimizer Accuracy Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final performance summary\n",
    "print(\"\\nüéØ Final Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "optimizers = ['Vanilla GD', 'Momentum GD', 'Adam']\n",
    "final_losses = [losses_vanilla[-1], losses_momentum[-1], losses_adam[-1]]\n",
    "final_accuracies = [accuracies_vanilla[-1], accuracies_momentum[-1], accuracies_adam[-1]]\n",
    "\n",
    "for opt, loss, acc in zip(optimizers, final_losses, final_accuracies):\n",
    "    print(f\"{opt:<15} Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(\"- Adam typically converges fastest and most reliably\")\n",
    "print(\"- Momentum helps with ravine-like loss surfaces\")\n",
    "print(\"- Vanilla GD is simplest but may need careful tuning\")\n",
    "print(\"- Different optimizers may work better for different problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** Comparative analysis complete.\n",
    "\n",
    "- **Performance hierarchy:** Adam > Momentum > Vanilla GD for most problems\n",
    "- **Convergence patterns:** Adaptive methods show more stable optimization\n",
    "- **Problem dependence:** No single optimizer works best for all scenarios\n",
    "\n",
    "**Healthcare Translation:** Like choosing treatment protocols - Adam works for most cases, but specialized approaches needed for specific conditions.\n",
    "\n",
    "## üéØ Key Takeaways and Nigerian Healthcare Applications\n",
    "\n",
    "**Algorithm Summary:**\n",
    "\n",
    "- **Vanilla GD:** Simple foundation, slow but reliable for convex problems\n",
    "- **Momentum:** Physics-inspired acceleration, helps escape local minima\n",
    "- **Adam:** State-of-the-art optimizer combining momentum with adaptivity\n",
    "- **Selection criteria:** Problem complexity, computational resources, convergence requirements\n",
    "\n",
    "**Healthcare Translation - Mark:**\n",
    "\n",
    "Imagine training AI for Nigerian hospitals:\n",
    "\n",
    "- **Adam optimizer:** Default choice for most deep learning medical models\n",
    "- **Momentum:** Good for simpler models with clear loss landscapes\n",
    "- **Vanilla GD:** Useful for understanding optimization fundamentals\n",
    "- **Adaptive learning:** Critical for handling variable patient data patterns\n",
    "\n",
    "**Performance achieved:** All optimizers successfully trained logistic regression models with high accuracy!\n",
    "\n",
    "**Reflection Questions:**\n",
    "\n",
    "1. Why has Adam become the default optimizer in deep learning?\n",
    "\n",
    "2. How might different optimizers affect medical AI reliability?\n",
    "\n",
    "3. Compare optimization to how doctors refine their diagnostic approaches.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Apply these optimizers to neural network training\n",
    "- Explore learning rate scheduling techniques\n",
    "- Investigate second-order optimization methods\n",
    "\n",
    "**üèÜ Excellent progress, my student! You've mastered the optimization algorithms that power modern AI.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
